{"meta":{"title":"小彩鸟","subtitle":"一起成长","description":"一起成长","author":"Clouduncle","url":"https://blog.utone.xyz","root":"/"},"pages":[{"title":"","date":"2020-05-24T11:58:18.542Z","updated":"2019-11-21T12:26:18.000Z","comments":true,"path":"404.html","permalink":"https://blog.utone.xyz/404.html","excerpt":"","text":""},{"title":"About","date":"2015-07-16T04:00:00.000Z","updated":"2020-05-25T00:04:39.638Z","comments":true,"path":"about/index.html","permalink":"https://blog.utone.xyz/about/index.html","excerpt":"","text":""},{"title":"Archives","date":"2017-09-20T12:49:56.000Z","updated":"2019-11-21T12:26:18.000Z","comments":false,"path":"archive/index.html","permalink":"https://blog.utone.xyz/archive/index.html","excerpt":"","text":""},{"title":"search","date":"2020-05-24T15:41:20.797Z","updated":"2020-05-24T15:40:11.568Z","comments":true,"path":"search/index.html","permalink":"https://blog.utone.xyz/search/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-05-24T12:46:29.221Z","updated":"2020-05-24T12:46:29.221Z","comments":true,"path":"tags/index.html","permalink":"https://blog.utone.xyz/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"flink","slug":"flink/flink","date":"2019-02-18T04:00:00.000Z","updated":"2020-06-13T11:56:10.480Z","comments":true,"path":"2019/02/18/flink/flink/","link":"","permalink":"https://blog.utone.xyz/2019/02/18/flink/flink/","excerpt":"","text":"flink介绍&emsp;&emsp;Flink 是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。并且Flink 提供了数据分布、容错机制以及资源管理等核心功能。Flink 提供了诸多高抽象层的API 以便用户编写分布式任务：&emsp;&emsp;DataSet API， 对静态数据进行批处理操作，将静态数据抽象成分布式的数据集，用户可以方便地使用Flink提供的各种操作符对分布式数据集进行处理，支持Java、Scala和Python。&emsp;&emsp;DataStream API，对数据流进行流处理操作，将流式的数据抽象成分布式的数据流，用户可以方便地对分布式数据流进行各种操作，支持 Java 和 Scala。&emsp;&emsp;Table API，对结构化数据进行查询操作，将结构化数据抽象成关系表，并通过类 SQL 的DSL 对关系表进行各种查询操作，支持 Java 和 Scala。 flink运行时组件 JobManager TaskManager Flink中的工作进程。通常在flink中会有多个TaskManager运行，每一个TaskManager都包含了一定数量的插槽（slots）。插槽的数量限制了TaskManager能够执行的任务数量也就是说最大并发能力是由slots数量限制的。 启动之后，TaskManager会向资源管理器注册它的插槽；收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给JobManager调用。JobManager就可以向插槽分配任务（task）来执行。 在执行过程中，一个TaskManager可以跟其他运行同一应用程序的TaskManager交换数据。 flink中每个TaskManager都是一个jvm进程（process），它可能会在独立的线程上执行一个或多个子任务 为了控制一个TaskManager能接收多少个task，TaskManager通过task slot来进行控制（一个TaskManager至少有一个slot） 在slot上执行的task是一个线程，每个线程都需要执行在固定的计算资源上，这个资源就是slot。比如TaskManager有三个slot，那么它就会把自己的内存分成三份给每个slot，所以slot之间内存是独享的但是cpu不是独享的，所以建议slot数量和cpu个数设置一样 默认情况下，Flink允许子任务共享slot，即使他们是不同任务的子任务。这样可以提高资源利用率 TM的数量和slot数量，决定了并行处理的最大能力。但是不一定程序执行时一定都用到。程序执行时的并行度才是用到的能力 ResourceManager 主要负责管理任务管理器（TaskManager）的插槽（slots），插槽是flink中定义的处理资源单元。 Flink为不同的环境和资源管理工具提供了不同的资源管理器，比如Yarn、K8s、Mesos以及standalone部署 当JobManager申请插槽资源时，ResourceManager会将有空闲插槽的TaskManager分配给JobManager。如果ResourceManager没有足够的插槽来满足JobManager的请求，它还可以向资源提供平台发起会话，以提供启动TaskManager进程的容器 DisPatcher(分发器) 可以跨作业运行，它为应用提交提供了REST接口。 当一个应用被提交执行时，分发器就会启动并将应用提交给一个JobManager DisPatcher也会启动一个Web UI，用来方便的展示和监控作业执行的信息 DisPatcher在架构中可能并不是必需的，这取决于应用提交运行的方式 提交任务流程standalone模式任务提交流程 yarn模式提交流程","categories":[],"tags":[{"name":"flink","slug":"flink","permalink":"https://blog.utone.xyz/tags/flink/"}]},{"title":"生产HDFS Block损坏恢复最佳实践","slug":"hadoop/HDFS Block损坏恢复","date":"2017-12-02T04:00:00.000Z","updated":"2020-05-28T00:12:39.893Z","comments":true,"path":"2017/12/02/hadoop/HDFS Block损坏恢复/","link":"","permalink":"https://blog.utone.xyz/2017/12/02/hadoop/HDFS%20Block%E6%8D%9F%E5%9D%8F%E6%81%A2%E5%A4%8D/","excerpt":"","text":"HDFS是比较稳定的,95%的情况下不会遇到这种情况 文件bigdata.md12345678910111213141516171819202122232425262728293031323334上传:-bash-4.2$ hdfs dfs -mkdir &#x2F;blockrecover-bash-4.2$ echo &quot;www.ruozedata.com&quot; &gt; bigfdata.md-bash-4.2$ hdfs dfs -put bigfdata.md &#x2F;blockrecover-bash-4.2$ hdfs dfs -ls &#x2F;blockrecoverFound 1 items-rw-r--r-- 3 hdfs supergroup 18 2019-03-03 14:42 &#x2F;blockrecover&#x2F;bigfdata.md-bash-4.2$ 校验: 健康状态-bash-4.2$ hdfs fsck &#x2F;Connecting to namenode via http:&#x2F;&#x2F;yws76:50070&#x2F;fsck?ugi&#x3D;hdfs&amp;path&#x3D;%2FFSCK started by hdfs (auth:SIMPLE) from &#x2F;192.168.0.76 for path &#x2F; at Sun Mar 03 14:44:44 CST 2019...............................................................................Status: HEALTHY Total size: 50194618424 B Total dirs: 354 Total files: 1079 Total symlinks: 0 Total blocks (validated): 992 (avg. block size 50599413 B) Minimally replicated blocks: 992 (100.0 %) Over-replicated blocks: 0 (0.0 %) Under-replicated blocks: 0 (0.0 %) Mis-replicated blocks: 0 (0.0 %) Default replication factor: 3 Average block replication: 3.0 Corrupt blocks: 0 Missing replicas: 0 (0.0 %) Number of data-nodes: 3 Number of racks: 1FSCK ended at Sun Mar 03 14:44:45 CST 2019 in 76 millisecondsThe filesystem under path &#39;&#x2F;&#39; is HEALTHY-bash-4.2$ 直接DN节点上删除文件一个block的一个副本(3副本)123456789101112131415161718192021222324252627282930删除块和meta文件:[root@yws87 subdir135]# rm -rf blk_1075808214 blk_1075808214_2068515.meta直接重启HDFS，直接模拟损坏效果，然后fsck检查:-bash-4.2$ hdfs fsck &#x2F;Connecting to namenode via http:&#x2F;&#x2F;yws77:50070&#x2F;fsck?ugi&#x3D;hdfs&amp;path&#x3D;%2FFSCK started by hdfs (auth:SIMPLE) from &#x2F;192.168.0.76 for path &#x2F; at Sun Mar 03 16:02:04 CST 2019.&#x2F;blockrecover&#x2F;bigfdata.md: Under replicated BP-1513979236-192.168.0.76-1514982530341:blk_1075808214_2068515. Target Replicas is 3 but found 2 live replica(s), 0 decommissioned replica(s), 0 decommissioning replica(s)................................................................................Status: HEALTHY Total size: 50194618424 B Total dirs: 354 Total files: 1079 Total symlinks: 0 Total blocks (validated): 992 (avg. block size 50599413 B) Minimally replicated blocks: 992 (100.0 %) Over-replicated blocks: 0 (0.0 %) Under-replicated blocks: 1 (0.10080645 %) Mis-replicated blocks: 0 (0.0 %) Default replication factor: 3 Average block replication: 2.998992 Corrupt blocks: 0 Missing replicas: 1 (0.033602152 %) Number of data-nodes: 3 Number of racks: 1FSCK ended at Sun Mar 03 16:02:04 CST 2019 in 148 millisecondsThe filesystem under path &#39;&#x2F;&#39; is HEALTHY-bash-4.2$ 手动修复hdfs debug (这个命令并不一定成功，手动修复看运气)1234567891011121314151617181920-bash-4.2$ hdfs |grep debug没有输出debug参数的任何信息结果！故hdfs命令帮助是没有debug的，但是确实有hdfs debug这个组合命令，切记。修复命令:-bash-4.2$ hdfs debug recoverLease -path &#x2F;blockrecover&#x2F;bigfdata.md -retries 10recoverLease SUCCEEDED on &#x2F;blockrecover&#x2F;bigfdata.md-bash-4.2$ 直接DN节点查看，block文件和meta文件恢复:[root@yws87 subdir135]# lltotal 8-rw-r--r-- 1 hdfs hdfs 56 Mar 3 14:28 blk_1075808202-rw-r--r-- 1 hdfs hdfs 11 Mar 3 14:28 blk_1075808202_2068503.meta[root@yws87 subdir135]# lltotal 24-rw-r--r-- 1 hdfs hdfs 56 Mar 3 14:28 blk_1075808202-rw-r--r-- 1 hdfs hdfs 11 Mar 3 14:28 blk_1075808202_2068503.meta-rw-r--r-- 1 hdfs hdfs 18 Mar 3 15:23 blk_1075808214-rw-r--r-- 1 hdfs hdfs 11 Mar 3 15:23 blk_1075808214_2068515.meta 自动修复123456789当数据块损坏后，DN节点执行directoryscan操作之前，都不会发现损坏；也就是directoryscan操作是间隔6hdfs.datanode.directoryscan.interval : 21600在DN向NN进行blockreport前，都不会恢复数据块;也就是blockreport操作是间隔6hdfs.blockreport.intervalMsec : 21600000当NN收到blockreport才会进行恢复操作。 总结生产上本人一般倾向于使用 手动修复方式，但是前提要手动删除损坏的block块。 切记，是删除损坏block文件和meta文件，而不是删除hdfs文件。 当然还可以先把文件get下载，然后hdfs删除，再对应上传。 切记删除不要执行: hdfs fsck / -delete 这是删除损坏的文件， 那么数据不就丢了嘛；除非无所谓丢数据，或者有信心从其他地方可以补数据到hdfs！ 思考题那么如何确定一个文件的损失的块位置，哪几种方法呢？CDH的配置里搜索没有这两个参数，怎么调整生效呢？","categories":[],"tags":[{"name":"故障","slug":"故障","permalink":"https://blog.utone.xyz/tags/%E6%95%85%E9%9A%9C/"},{"name":"hdfs","slug":"hdfs","permalink":"https://blog.utone.xyz/tags/hdfs/"}]},{"title":"sqoop","slug":"sqoop/sqoop","date":"2017-10-23T04:00:00.000Z","updated":"2020-06-07T09:10:21.067Z","comments":true,"path":"2017/10/23/sqoop/sqoop/","link":"","permalink":"https://blog.utone.xyz/2017/10/23/sqoop/sqoop/","excerpt":"","text":"介绍&emsp;&emsp;用于Hadoop和RDBMS之间进行数据传输的一个工具而已&emsp;&emsp;参考点：进Hadoop就是导入 从Hadoop出去的就是导出 工作机制&emsp;&emsp;底层是使用MapReduce实现，只有Mapper没有Reducer的，Sqoop的作业最终还是要提交到YARN上去执行的 导入mysql-&gt; hdfs 全量导入12345678910111213bin&#x2F;sqoop import \\--connect jdbc:mysql:&#x2F;&#x2F;hadoop102:3306&#x2F;company \\--username root \\--password 110011 \\--table emp \\--target-dir &#x2F;user&#x2F;company \\--delete-target-dir \\--mapreduce-job-name ll \\--m 4 \\--split-by deptno \\--fields-terminated-by &quot;\\t&quot; \\--null-string &quot;0&quot; \\--null-non-string &quot;0&quot; 注: – connect jdbc,–username,–password， –table，指mysql中的表 –target-dir，sqoop会在hdfs上自动创建这个目录 –delete-target-dir，如果target-dir目录在hdfs上已经存在就会报错，加上这句话就是如果存在就提前删除目录 –fields-terminated-by “\\t”，导入hdfs存储后列字段之间分隔符格式为”\\t”,如果不加默认是’,’ –m，指定mapper个数 –split-by，如果mysql的表中没有设置主键，会报错。解决方法：①将map个数设置为1 ②–split-by后面跟上表列的名字，最好不是非数字类型的。当m！=1时，–split-by默认是主键，如果没有主键会报错。首先sqoop会向关系型数据库比如mysql发送一个命令:select max(id),min(id) from test。然后会把max、min之间的区间平均分为–m份，最后–m个并行的map去找数据库,最后可能在hdfs上会生成空文件即通过–-split-by后面字段划分的多个区间中,某一个或几个区间在mysql中没有对应值所以在hdfs中生成空文件。–split-by对非数字类型的字段支持不好。一般用于主键及数字类型的字段 –null-string，导入string类型的列如果null，替换为指定字符串。如果不加在mysql是null，在hdfs存储是字符串null –null-non-string，导入非string类型的列如果null，替换为指定字符串 指定列指定条件导入123456789101112131415bin&#x2F;sqoop import \\--connect jdbc:mysql:&#x2F;&#x2F;hadoop102:3306&#x2F;company \\--username root \\--password 110011 \\--table emp \\--columns &quot;EMPNO,ENAME,JOB,SAL,COMM&quot; \\--where &#39;SAL&gt;2000&#39; \\--target-dir &#x2F;user&#x2F;company \\--delete-target-dir \\--mapreduce-job-name ll \\--m 4 \\--split-by deptno \\--fields-terminated-by &quot;\\t&quot; \\--null-string &quot;0&quot; \\--null-non-string &quot;0&quot; 查询导入12345678910111213bin&#x2F;sqoop import \\--connect jdbc:mysql:&#x2F;&#x2F;hadoop102:3306&#x2F;company \\--username root \\--password 110011 \\--target-dir &#x2F;user&#x2F;company \\--delete-target-dir \\--mapreduce-job-name ll \\--m 4 \\--split-by job \\--fields-terminated-by &quot;\\t&quot; \\--null-string &quot;0&quot; \\--null-non-string &quot;0&quot; \\--query &#39;select empno,ename,job from emp where sal &gt; 1500 and $CONDITIONS&#39; 注: –query选项不能与–table选项同时使用。 –query使用时必须伴随参–target-dir –split-by后面的列要在select列里面找 在where语句中必须包含’$CONDITIONS’ mysql-&gt; hive在hive中手动创建表,不要用自动创建表参数会改变字段类型 方法一1234567891011bin&#x2F;sqoop import \\--connect jdbc:mysql:&#x2F;&#x2F;hadoop102:3306&#x2F;company \\--username root \\--password 110011 \\--fields-terminated-by &quot;\\t&quot; \\--target-dir &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;emp&#x2F;day&#x3D;20180206 \\--delete-target-dir \\--mapreduce-job-name ll \\--m 4 \\--split-by job \\--query &#39;select empno,ename,job,sal,deptno from emp where sal &gt; 1500 and $CONDITIONS&#39; 注: 此种方法只能导入非分区hive表。如果是分区hive表的话是查不到数据的，还需要在hive中执行msck repair table emp 方法二12345678910111213141516bin&#x2F;sqoop import \\--connect jdbc:mysql:&#x2F;&#x2F;hadoop102:3306&#x2F;company \\--username root \\--password 110011 \\--target-dir &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;emp&#x2F;day&#x3D;20181117 \\--delete-target-dir \\--hive-import \\--fields-terminated-by &quot;\\t&quot; \\--mapreduce-job-name mysql_to_hive \\--m 1 \\--split-by job \\--hive-partition-key day \\--hive-partition-value 20181117 \\--hive-database default \\--hive-table emp \\--query &#39;select empno,ename,job,sal,deptno from emp where sal &gt; 1500 and $CONDITIONS&#39; –hive-overwrite –hive-table staff_hive注: –hive-import将数据从关系数据库中导入到hive表中 hive-partition-key创建分区，后面直接跟分区名，分区字段的默认类型为string –hive-partition-value导入数据时，指定某个分区的值 –hive-database –hive-table 导出hdfs -&gt; mysql123456789101112bin&#x2F;sqoop export \\--connect &quot;jdbc:mysql:&#x2F;&#x2F;hadoop102:3306&#x2F;company?useUnicode&#x3D;true&amp;characterEncoding&#x3D;utf-8&quot; \\--username root \\--password 110011 \\--table emp_demp \\--m 4 \\--export-dir &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;emp&#x2F;day&#x3D;20181117 \\--input-fields-terminated-by &quot;\\t&quot; \\--update-mode allowinsert \\--update-key &quot;ename&quot; \\--input-null-string &#39;&#39; \\--input-null-non-string &#39;&#39; \\ 注: –update-mode 如果不加这个参数,默认是append数据,不是覆盖原先数据 updateonly 只更新，无法插入新数据..就是只能更新mysql原来已有的数据 allowinsert 允许新增 已存在更新,未存在的插入.前提是表中有主键,如果没有主键那么就是全部插入 –update-key：允许更新的情况下，指定哪些字段匹配视为同一条数据，进行更新而不增加。多个字段用逗号分隔。对比mysql中数据,如果update-key里的字段和mysql里对应的字段都一样就认为一样则更新这条数据,否则就是新增 123456789101112bin&#x2F;sqoop export \\--connect &quot;jdbc:mysql:&#x2F;&#x2F;hadoop102:3306&#x2F;company?useUnicode&#x3D;true&amp;characterEncoding&#x3D;utf-8&quot; \\--username root \\--password 110011 \\--table emp_demp \\--m 4 \\--export-dir &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;emp&#x2F;day&#x3D;20181117 \\--input-fields-terminated-by &quot;\\t&quot; \\--input-null-string &#39;&#39; \\--input-null-non-string &#39;&#39; \\--staging-table emp_tmp \\--clear-staging-table 注: –staging-table emp_tmp 和–clear-staging-table 不能和–update-mode一起使用 测试用建表语句123456789101112131415161718192021222324252627282930313233343536373839404142drop table if exists emp;create table emp ( empno string, ename string, job string, sal decimal(10,2), deptno string)partitioned by (day string)row format delimited fields terminated by &#39;\\t&#39;create table emp ( empno numeric(4) not null, ename varchar(10), job varchar(9), mgr numeric(4), hiredate datetime, sal numeric(7, 2), comm numeric(7, 2), deptno numeric(2));insert into emp values (7369, &#39;SMITH&#39;, &#39;CLERK&#39;, 7902, &#39;1980-12-17&#39;, 800, null, 20);insert into emp values (7499, &#39;ALLEN&#39;, &#39;SALESMAN&#39;, 7698, &#39;1981-02-20&#39;, 1600, 300, 30);insert into emp values (7521, &#39;WARD&#39;, &#39;SALESMAN&#39;, 7698, &#39;1981-02-22&#39;, 1250, 500, 30);insert into emp values (7566, &#39;JONES&#39;, &#39;MANAGER&#39;, 7839, &#39;1981-04-02&#39;, 2975, null, 20);insert into emp values (7654, &#39;MARTIN&#39;, &#39;SALESMAN&#39;, 7698, &#39;1981-09-28&#39;, 1250, 1400, 30);insert into emp values (7698, &#39;BLAKE&#39;, &#39;MANAGER&#39;, 7839, &#39;1981-05-01&#39;, 2850, null, 30);insert into emp values (7782, &#39;CLARK&#39;, &#39;MANAGER&#39;, 7839, &#39;1981-06-09&#39;, 2450, null, 10);insert into emp values (7788, &#39;SCOTT&#39;, &#39;ANALYST&#39;, 7566, &#39;1982-12-09&#39;, 3000, null, 20);insert into emp values (7839, &#39;KING&#39;, &#39;PRESIDENT&#39;, null, &#39;1981-11-17&#39;, 5000, null, 10);insert into emp values (7844, &#39;TURNER&#39;, &#39;SALESMAN&#39;, 7698, &#39;1981-09-08&#39;, 1500, 0, 30);insert into emp values (7876, &#39;ADAMS&#39;, &#39;CLERK&#39;, 7788, &#39;1983-01-12&#39;, 1100, null, 20);insert into emp values (7900, &#39;JAMES&#39;, &#39;CLERK&#39;, 7698, &#39;1981-12-03&#39;, 950, null, 30);insert into emp values (7902, &#39;FORD&#39;, &#39;ANALYST&#39;, 7566, &#39;1981-12-03&#39;, 3000, null, 20);insert into emp values (7934, &#39;MILLER&#39;, &#39;CLERK&#39;, 7782, &#39;1982-01-23&#39;, 1300, null, 10);create table emp_tmp as select empno,ename,job,sal,deptno from emp where 1&#x3D;2;","categories":[],"tags":[{"name":"sqoop","slug":"sqoop","permalink":"https://blog.utone.xyz/tags/sqoop/"}]},{"title":"git","slug":"git/git","date":"2017-05-18T04:00:00.000Z","updated":"2020-06-22T09:54:06.726Z","comments":true,"path":"2017/05/18/git/git/","link":"","permalink":"https://blog.utone.xyz/2017/05/18/git/git/","excerpt":"","text":"git工作流程 从远程仓库中克隆 Git 资源作为本地仓库。 从本地仓库中 checkout 代码然后进行代码修改 在提交前先将代码提交到暂存区。 提交修改。提交到本地仓库。本地仓库中保存修改的各个历史版本。 在修改完成后，需要和团队成员共享代码时，可以将代码 push 到远程仓库。 git安装下载官网地址：https://git-scm.com/下载地址： https://git-scm.com/download 安装git 安装TortoiseGit官网可以下载安装包和汉化包官网：https://tortoisegit.org/download/ 安装 TortoiseGit 中文包安装中文语言包并不是必选项。完全根据个人喜好来选择安装任意位置鼠标右键选择 Settings 基本操作创建版本库&emsp;&emsp;版本库又名仓库，英文名 repository，你可以简单理解成一个目录，这个目录里面的所有文件都可以被 Git 管理起来，每个文件的修改、删除，Git 都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原”。由于 git 是分布式版本管理工具，所以 git 在不需要联网的情况下也具有完整的版本管理能力。&emsp;&emsp;创建一个版本库非常简单，可以使用 git bash 也可以使用 tortoiseGit。首先，选择一个合适的地方，创建一个空目录。 方式一使用 GitBash&emsp;&emsp;在当前目录中点击右键中选择 Git Bash Here，打开命令窗口。输入 git init 命令，回车如下如所示，即创建好了一个空的 git 仓库 方式二使用 TortoiseGit&emsp;&emsp;只需要在目录中点击右键菜单选择“在这里创建版本库” 版本库：“.git”目录就是版本库，将来文件都需要保存到版本库中。 工作目录：包含“.git”目录的目录，也就是.git 目录的上一级目录就是工作目录。只有工作目录中的文件才能保存到版本库中。","categories":[],"tags":[{"name":"git","slug":"git","permalink":"https://blog.utone.xyz/tags/git/"}]},{"title":"Hive Sql","slug":"hive/Hive SQL","date":"2016-07-12T04:00:00.000Z","updated":"2020-06-04T02:34:05.724Z","comments":true,"path":"2016/07/12/hive/Hive SQL/","link":"","permalink":"https://blog.utone.xyz/2016/07/12/hive/Hive%20SQL/","excerpt":"","text":"hive sql语法case whensum(if()) between 800 and 1500 左闭右闭的[800,1500]limitin/not inis null聚合函数 ： max/min/count/sum/avg不管是sql还是代码业务逻辑:一定要考虑全面(数据为空可能是””,’’,null),所以需要进行单元测试 group by ： 在select里面出现的字段，要么出现在group by里面，要么就出现在聚合函数里面 where,group by,having,order by同时使用，执行顺序为 （1）where过滤数据（2）对筛选结果集group by分组（3）对每个分组进行select查询，提取对应的列，有几组就执行几次（4）再进行having筛选每组数据（5）最后整体进行order by排序 hive joinjoin : 保留两表交集数据left join : 保留左表数据,右表关联不上的为nullright join : 保留右表数据,左表关联不上的为nullfull outer join : 两表数据全部保留,左或右没关联上的字段为nullleft semi join : LEFT SEMI JOIN （左半连接）主要是用来代替in的，是 IN/EXISTS 子查询的一种更高效的实现。 12345SELECT a.key, a.value FROM a WHERE a.key in (SELECT b.key FROM B); 可以改写为： 12SELECT a.key, a.val FROM a LEFT SEMI JOIN b ON (a.key &#x3D; b.key) 1、left semi join 的限制是，JOIN子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行。2、left semi join 是只传递表的 join key 给 map 阶段，因此left semi join 中最后 select 的结果只许出现左表。3、因为 left semi join 是 in(keySet) 的关系，遇到右表重复记录，左表会跳过，而 join 则会一直遍历。这就导致右表有重复值得情况下 left semi join 只产生一条，join 会产生多条，所以 left semi join 的性能更高。 https://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference/LanguageManual_Joins.html#joins hive function查询hive自带的函数:show functions like ‘date‘ 内置函数具体功能介绍 : desc function extended function_name 窗口函数12345678910111213141516171819202122232425rank( ) : 排序相同时会重复,总数不会变 --1,2,3,3,5dense_rank() : 排序相同时会重复,总数会减少 --1,2,3,3,4row_number() : 会根据顺序计算lag(col,n) : 往前数第n行数据lead(col,n) : 往后第n行数据网上解释:over(partition by xx order by xx) --默认从起始点到当前行over(partition by xx) --分组内所有行over(partition by xx order by xx ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) --默认从起始点到当前行over(partition by xx order by xx ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) --当前行+往前3行over(partition by xx order by xx ROWS BETWEEN 3 PRECEDING AND 1 FOLLOWING) --当前行+往前3行+往后1行over(partition by xx order by xx ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) --当前行 + 往后所有行 判空nvl()12nvl（expr1，expr2）若expr1为Null，则返回expr2，否则返回expr1。 内置时间相关的current_data12345hive (default)&gt; select current_date;OK_c02016-07-12Time taken: 0.169 seconds, Fetched: 1 row(s) current_timestamp获取当前unix时间戳 1234hive (default)&gt; select current_timestamp;OK_c02016-07-12 23:16:41.237 unix_timestamp()转化unix时间戳到当前时区的时间格式 1234567891011121314151617hive (default)&gt; select unix_timestamp();OK_c01591024657---------------------------------------------------------------hive (default)&gt; select unix_timestamp(&quot;2016-07-12 20:09:00&quot;,&quot;yyyy-MM-dd HH:mm:ss&quot;);OK_c01468325340---------------------------------------------------------------hive (default)&gt; select (unix_timestamp(&#39;2018-05-25 12:03:55&#39;) - unix_timestamp(&#39;2018-05-25 11:03:55&#39;))&#x2F;3600;OK_c01.0 from_unixtime()1234567891011hive (default)&gt; select from_unixtime(1468325340,&quot;yyyy-MM-dd HH:mm:ss&quot;);OK_c02016-07-12 20:09:00------------------------------------20161205 -&gt;(转换)-&gt;2016-12-05hive (default)&gt; select from_unixtime(unix_timestamp(&#39;20161205&#39;,&#39;yyyymmdd&#39;),&#39;yyyy-mm-dd&#39;);OK_c02016-12-05 to_date()日期时间转日期函数 1234hive (default)&gt; select to_date(&#39;2016-11-11 12:33:42&#39;);OK_c02016-11-11 year()/month()/day()/hour()/minute()/second()1234hive (default)&gt; select year(&#39;2016-07-12 20:09:00&#39;),year(&#39;1591024657&#39;),month(&#39;2016-07-12 20:09:00&#39;),day(&#39;2016-07-12 20:09:00&#39;);OK_c0 _c1 _c2 _c32016 NULL 7 12 date_add()1234hive (default)&gt; select date_add(&#39;2016-11-11 12:22:34&#39;,1);OK_c02016-11-12 date_sub()1234hive (default)&gt; select date_sub(&#39;2016-11-11 12:22:34&#39;,1);OK_c02016-11-10 datediff()1234hive (default)&gt; SELECT datediff(&#39;2009-07-31&#39;, &#39;2009-07-30&#39;);OK_c01 next_day123452016-11-11当天下周一的日期hive (default)&gt; select next_day(&#39;2016-11-11&#39;,&#39;MO&#39;);OK_c02016-11-14 weekofyear：返回日期在当前周数1234hive (default)&gt; SELECT weekofyear(&#39;2008-02-20&#39;);OK_c08 hive 计算某一个日期属于星期几1234hive (default)&gt; SELECT IF(pmod(datediff(&#39;2018-05-20&#39;, &#39;1920-01-01&#39;) - 3, 7)&#x3D;&#39;0&#39;, 7, pmod(datediff(&#39;2018-05-20&#39;, &#39;1920-01-01&#39;) - 3, 7)) ;OK_c07 内置数学相关round()1234hive (default)&gt; SELECT round(12.3456, 1);OK_c012.3 ceil()1234567891011hive (default)&gt; SELECT ceil(-0.1);OK_c00-----------------------------------------------hive (default)&gt; SELECT ceil(5.2);OK_c06 floor()12345678910hive (default)&gt; SELECT floor(-0.1);OK_c0-1-----------------------hive (default)&gt; SELECT floor(3.6);OK_c03 abs()绝对值 字符串相关:substr()123456&gt; SELECT substr(&#39;Facebook&#39;, 5) FROM src LIMIT 1;&#39;book&#39;&gt; SELECT substr(&#39;Facebook&#39;, -5) FROM src LIMIT 1;&#39;ebook&#39;&gt; SELECT substr(&#39;Facebook&#39;, 5, 1) FROM src LIMIT 1;&#39;b&#39; concat()12&gt; SELECT concat(&#39;abc&#39;, &#39;def&#39;);&#39;abcdef&#39; concat_ws()1234hive (default)&gt; SELECT concat_ws(&#39;.&#39;, &#39;www&#39;, array(&#39;facebook&#39;, &#39;com&#39;),&#39;ddd&#39;);OK_c0www.facebook.com.ddd length()12&gt; SELECT length(&#39;Facebook&#39;);8 regexp_replace()1234hive (default)&gt; SELECT regexp_replace(&#39;2019-11-11&#39;,&#39;-&#39;,&#39;,&#39;);OK_c02019,11,11 split()1234567891011hive (default)&gt; SELECT split(&#39;oneAtwoBthreeC&#39;, &#39;[ABC]&#39;) ;OK_c0[&quot;one&quot;,&quot;two&quot;,&quot;three&quot;,&quot;&quot;]-----------------------------------------------------------------hive (default)&gt; SELECT split(&#39;oneAtwoBthreeC&#39;, &#39;A&#39;) ;OK_c0[&quot;one&quot;,&quot;twoBthreeC&quot;] upper()1234hive (default)&gt; SELECT upper(&#39;Facebook&#39;);OK_c0FACEBOOK lower()1234hive (default)&gt; SELECT lower(&#39;Facebook&#39;);OK_c0facebook 行转列1234567891011121314151617hive (default)&gt; select explode(array(1,2,3));OKcol123--------------------------------------------------------select GET_JSON_OBJECT( event_view.event_json,&#39;$.entry&#39;) entry, GET_JSON_OBJECT( event_view.event_json,&#39;$.loading_time&#39;) loading_time, GET_JSON_OBJECT( event_view.event_json,&#39;$.action&#39;) action, GET_JSON_OBJECT( event_view.event_json,&#39;$.open_ad_type&#39;) open_ad_type, GET_JSON_OBJECT( event_view.event_json,&#39;$.detail&#39;) detail , event_view.event_timefrom ods_base_logLATERAL VIEW FlatEventUDTF(GET_JSON_OBJECT(log_string,&#39;$.et&#39; )) event_view as event_time,event_name,event_json jsonjson_tuple1select json_tuple(json,&#39;movie&#39;,&#39;rate&#39;,&#39;time&#39;,&#39;userid&#39;) as(movie,rate,time,userid) from rating_json get_json_object1GET_JSON_OBJECT(log_string,&#39;$.cm.mid&#39;) mid 分组topN每个性别中年龄最大的2个人 1234567select id,age,name,sexfrom(select id,age,name,sex,row_number() over(partition by sex order by age desc) as rankfrom hive_topn) t where rank&lt;&#x3D;2; 结束","categories":[],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://blog.utone.xyz/tags/Hive/"}]},{"title":"Hive","slug":"hive/Hive","date":"2016-07-10T04:00:00.000Z","updated":"2020-06-01T14:18:40.593Z","comments":true,"path":"2016/07/10/hive/Hive/","link":"","permalink":"https://blog.utone.xyz/2016/07/10/hive/Hive/","excerpt":"","text":"Hive介绍&emsp;&emsp;Hive是基于Hadoop的一个数据仓库工具，可以将结构化数据文件映射为一张表，并提供类SQL查询功能，Hive的sql叫HQL。Hive职责:将SQL转换为对应引擎的作业，Hive的数据存放在HDFS（源数据） + MySQL（元数据），想使用SQL进行查询Hadoop上的数据，源数据和元数据都是必不可少的。Hive就是个客户端而已,并不存在集群的概念，当然hive也可以部署多个，但是多个hive间是没有关系的。&emsp;&emsp;Hive的优势在于处理大数据,对于处理小数据没有优势，因为Hive的的适用场景是离线/批计算,不关注延时性 Hive架构 元数据&emsp;&emsp;Metastore：元数据包括报名，表所属数据库，表的拥有者，列/分区字段，表的类型（是否是外部表），表的数据所在的目录。默认存储在自带的derby数据库中，为了能多人访问使用Mysql存储元数据。注意和源数据（HDFS存的文本文件）区别。&emsp;&emsp;MetaStore是非常重要的,一旦这个挂了,HDFS源数据的元数据就没有了,所以要有HA的&emsp;&emsp;MetaStore是一个通用的组件:像SparkSQL/Flink/Impala/Presto,也是能访问Hive里面创建的表的,前提是这些框架能访问到hive的元数据mysql中的元数据表有30多种,如: DBS:存储数据库相关信息 TBLS:存储表的相关信息,包括表名和表的类型 COLUMNS_V2:字段表,保存表的字段名称,字段类型,描述,字段在表中的编号也就是列的位置 用户接口&emsp;&emsp;Client(就是hive的操作页面)包括：CLI就是命令行,JDBC(java访问hive) SQL Parser(解析器)&emsp;&emsp;将sql字符串转换成抽象语法树AST,对AST进行语法分析,比如表是否存在,字段是否存在,sql语义是否有误 Physical Plan(编译器)&emsp;&emsp;将AST编译生成逻辑执行计划 Query Optimizer(优化器)&emsp;&emsp;对逻辑执行计划进行优化 Execution(执行器)&emsp;&emsp;把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/TEZ/Spark （底层引擎有 : MR,Tez,Spark） HDFS&emsp;&emsp;HDFS用于存储数据 Hive和数据库比较&emsp;&emsp;hive数据是存储在hdfs上的，而数据库则将数据保存在本地文件系统中&emsp;&emsp;hive更多的是面对分析场景数据不会修改数据，mysql面对的是业务场景需要经常修改数据&emsp;&emsp;hive查询延迟高因为没有所以需要全表扫描而且如果底层引擎是MR那么MR本身延迟就很高，因为每个maptask，reducetask都是启动进程，mysql因为有索引所以查询延迟低（前提是数据规模较小，如果超出一定规模hive的并行计算显然能体现出优势）&emsp;&emsp;hive建立在集群上可以利用mapreduce的并行计算也可以很好地支持扩展（可扩展性），因此可以支持很大规模数据（数据规模）。而数据库可以支持的数据规模较小（目前hadoop集群最大规模是4000台左右，而数据库最大规模是100台左右） 部署123456789101112131415161718192021222324252627282930313233343536373839404142434445下载：http:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh5&#x2F;cdh&#x2F;5&#x2F; 在生产上对应的软件版本一定要控制好 版本选择的基本原则：尾巴一样(版本号)： cdh5.16.2 Hadoop&#x2F;Hive&#x2F;Sqoop&#x2F;HBase&#x2F;Oozie... Spark例外 所以Hive的版本：hive-1.1.0-cdh5.16.2 hive-1.1.0-cdh5.16.2-src.tar.gz 源码 hive-1.1.0-cdh5.16.2.tar.gz 安装包 wget http:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh5&#x2F;cdh&#x2F;5&#x2F;hive-1.1.0-cdh5.16.2.tar.gz解压：tar -zxvf hive-1.1.0-cdh5.16.2.tar.gz -C ~&#x2F;app&#x2F; 配置文件： conf Hive相关的配置文件 bin Hive相关的脚本 添加Hive的bin到环境变量 export HIVE_HOME&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hive-1.1.0-cdh5.16.2 export PATH&#x3D;$HIVE_HOME&#x2F;bin:$PATH 注意：改完一定要先source下，或者打开一个新的窗口修改配置文件 Hive的元数据是在MySQL里面的 如果要访问MySQL的话，需要如下几个参数 driver、url、user、password 还需要一个MySQL的驱动类 ***：不建议使用MySQL8的驱动 需要把MySQL的驱动拷贝到$HIVE_HOME&#x2F;lib Hive相关的配置参数均来自： https:&#x2F;&#x2F;cwiki.apache.org&#x2F;confluence&#x2F;display&#x2F;Hive&#x2F;Configuration+Propertieshive-env.sh HADOOP_HOME (如果环境变量里有不配置也可以)export HADOOP_HEAPSIZE&#x3D;1024(需要修改) 特别说明使用Hive时一定要确保Hadoop环境是OK(包括hdfs的nn处于安全模式也会出问题的) Hive配置参数显示数据库和表头hive-site.xml 里 1234567891011&lt;property&gt; &lt;!-- 显示所在数据库 --&gt; &lt;name&gt;hive.cli.print.header&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;!-- 打印表头信息 --&gt; &lt;name&gt;hive.cli.print.current.db&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; hive数据库存储路径hive创建数据库存放在HDFS的什么位置呢？规则：${hive.metastore.warehouse.dir}/数据库的名称.dbhive-site.xml 1hive.metastore.warehouse.dir 默认值:&#x2F;user&#x2F;hive&#x2F;warehouse Hive log存储路径Hive执行sql,控制台能看到的信息是有限的,所以需要看具体日志信息 Hive运行时的日志信息在哪里？Hive的log默认存放在/tmp/hadoop001(当前用户名)/hive.log目录下在hive-log4j.properties文件中修改log存放位置 如下:hive.log.dir= /opt/module/hive/logs (自己定义) 元数据存储到mysqlhive-site.xml 123456789101112131415161718192021&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt; &lt;value&gt;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;my_hive?createDatabaseIfNotExist&#x3D;true&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt; &lt;value&gt;root&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt; &lt;value&gt;root&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;&lt;&#x2F;configuration&gt; Hive里面涉及到参数的问题如何设置呢？1) hive-site.xml 全局配置2) set hive.cli.print.current.db=true; 局部设置仅对当前session有效&emsp;&emsp;set key; 显示key的值&emsp;&emsp;set key=value; 设置key的值为value如:&emsp;&emsp;set hive.cli.print.current.db=true;&emsp;&emsp;set hive.cli.print.header=true; 为什么不全都配置到全局的里面去呢？&emsp;&emsp;调优是针对作业的，不一定是针对全局的&emsp;&emsp;set key=value; //设置参数&emsp;&emsp;sql…&emsp;&emsp;set key=原值; //还原参数 ${java.io.tmpdir}: /tmp${user.name} : hadoop (用户名)${java.io.tmpdir}/${user.name} : /tmp/hadoop/hive.log (生产上可以改) Hive中的数据类型数值类型 ：int/float/double/bigint/decimal/boolean字符串类型：string 生产上时间和日期都是用的字符串类型 Hive基本命令交互式命令行: 直接在linux控制台上执行即可 hive -e “select * from student” hive -f 指定sql文件 shell脚本： hive -e “select * from student day=${day}” hive -f 指定sql文件 DDL(Data Definition Language)库创建数据库1234567891011CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name 避免要创建的数据库已经存在错误，增加if not exists判断[COMMENT database_comment][LOCATION hdfs_path] 指定数据库在HDFS上存放的位置[MANAGEDLOCATION hdfs_path][WITH DBPROPERTIES (property_name&#x3D;property_value, ...)];[]：表示可有可无(A|B):里面A和B可以任选其中一个 &#x3D;&#x3D;&#x3D;&#x3D;&gt;create database if not exists db_hive;create database if not exists db_hive2 location &#39;&#x2F;hive&#x2F;directory&#39;; hive不管是db,table,分区对应的都是hdfs上的文件夹类似于 【db文件夹/表文件夹/文件】 【db文件夹/表文件夹/分区文件夹(1..n)/文件】 查询数据库12show databases;show databases like &#39;db_hive*&#39;; 显示数据库信息1desc database db_hive; 使用数据库1use hive; 删除数据库123456如果删除的数据库不存在，最好采用 if exists判断数据库是否存在hive&gt; drop database if exists db_hive2;如果数据库不为空，可以采用cascade命令，强制删除hive&gt; drop database db_hive cascade;其实生产中都是创建数据库,修改&#x2F;删除(级联删除) 几乎是用不上的 表显示表1show tables &#39;*emp*&#39; 查看表结构命令123*desc formatted student;*能查看表路径,表的类型,源数据中列之间的分隔符 查看建表语句1show create table student; 查看表分区1show partitions student; 外部表和内部表12345678EXTERNAL：外部表 元数据被删除,HDFS数据依然存在 MANAGED：内部表 HDFS的数据被清空,元数据也会被清空生产中一般都是采用外部表的，好处就是如果误操作了把表删除了，但是数据还是在的，此时重新创建这个表又能查询了，因为重新创建这个表相当于又将元数据添加回来了，自然元数据和源数据就能对应上了 创建普通表1234567891011121314151617181920create external table bigdata_emp( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int) row format delimited fields terminated by &#39;\\t&#39;location &#39;&#x2F;hive&#x2F;external&#x2F;emp&#x2F;&#39;row format delimited fields terminated BY &#39;\\t&#39; 指定了列与列的分隔符为&#39;\\t&#39;或者:create table emp2 like bigdata_emp; 创建和bigdata_emp一样表结构的表,但是不会复制数据或者create table emp3 as select * from bigdata_emp; 创建和bigdata_emp一样表结构的表,而且也会复制进去数据 表在HDFS上的目录：${数据库的目录}/表名/有location那么就是location的文件夹位置:/hive/external/emp/(具体数据文件) 创建分区表分区意义:减少全表扫描的可能性,降低磁盘io, 1234567891011121314151617181920212223242526create table emp2( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double) PARTITIONED BY (day string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\\t&#39;;也可以用多级分区,如:create table emp2( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double) PARTITIONED BY (day string,hour string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\\t&#39;; 表在HDFS上的目录(比如按天分区)：${数据库的目录}/表名/day=20161211/${数据库的目录}/表名/day=20161212/ Hive的分区其实对应的还是Hdfs上的文件夹元数据里面也有一张对应的表存放分区信息的 修改表1ALTER TABLE bigdata_emp RENAME TO emp2_new; 删除表1234drop table if exists student;DROP和TRUNCATE的区别?TRUNCATE仅删除表中数据，保留表结构。Truncate只能删除管理表数据，不能删除外部表中数据 Hive加载数据在生产上使用： 1) load data2) insert (不是values) DML(Data Definition Language)load加载数据12345678910111213141516171819LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1&#x3D;val1, partcol2&#x3D;val2 ...)]LOCAL: 本地，你执行hive命令那个机器 无LOCAL：HDFS路径OVERWRITE：覆盖原先数据无OVERWRITE：在原先数据上追加例子：添加普通表load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;emp.txt&#39; overwrite into table bigdata_emp;添加分区表load data local inpath &#39;&#x2F;opt&#x2F;module&#x2F;data&#x2F;emp.txt&#39; overwrite into table emp2 partition(day&#x3D;&quot;20220808&quot;);或者hadoop fs -put &#x2F;opt&#x2F;module&#x2F;data&#x2F;emp.txt &#x2F;hive&#x2F;external&#x2F;emp&#x2F;这两种方法都能将数据放入对应的hive表中但是要注意hadoop fs -put这种方法,对分区表是不可以的比如:hadoop fs -put &#x2F;opt&#x2F;module&#x2F;data&#x2F;emp.txt &#x2F;hive&#x2F;external&#x2F;emp&#x2F;day&#x3D;20200809&#x2F; 此时查询select * from emp2 where day&#x3D;&#39;20200809&#39;此时虽然HDFS上的目录&#x2F;hive&#x2F;external&#x2F;emp&#x2F;day&#x3D;20200809&#x2F;是有数据的,但是元数据表中partition_key_values(记录表的分区信息)并没有最新分区20200809的信息,也就是hive metastore并没有感知这个分区的变化所以可以通过msck repair table emp2 ; 这样元数据信息里就有了20200809 同样也就能查询了 insert加载数据1234静态分区:insert into table emp2 partition(day&#x3D;&#39;20200810&#39;) select empno,ename,job,mgr,hiredate,sal,comm,deptno from bigdata_emp;动态分区: 需要设置非严格模式。插入时候不需要写具体分区,只需要写入分区字段就好。select里面的最后字段作为动态分区依据insert into table emp2 partition(day) select empno,ename,job,mgr,hiredate,sal,comm,day from bigdata_emp; 分隔符 log日志里面字段与字段之间的分隔符是什么 生产中用的比较多分隔符: ‘,’或\\t(Tab) 结束","categories":[],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://blog.utone.xyz/tags/Hive/"}]},{"title":"Yarn","slug":"hadoop/Yarn","date":"2016-07-02T04:00:00.000Z","updated":"2020-06-17T14:29:51.994Z","comments":true,"path":"2016/07/02/hadoop/Yarn/","link":"","permalink":"https://blog.utone.xyz/2016/07/02/hadoop/Yarn/","excerpt":"","text":"介绍yarn是一个资源调度框架，负责为运算程序 提供服务器 运算资源，总体上仍然是主从架构 Yarn并不清楚用户提交的程序的运行机制(只提供运算资源如需要cpu给你cpu，需要内存给你内存，而不管程序怎么跑) Yarn只提供运算资源的调度（用户程序向Yarn申请资源，Yarn就负责分配资源） Yarn中的主管角色叫ResourceManager Yarn中具体提供运算资源的角色叫NodeManager，App Master和Container都是运行在NM上 这样一来，Yarn其实就与运行的用户程序完全解耦成为一个通用的资源调度平台，就意味着Yarn上可以运行各种类型的分布式运算程序（mapreduce，spark等）。 生产中总内存的75%是给进程使用，25%系统保留（内存如果没有达到linux阈值，就不会执行oom-killer的机制），所以计算资源真正能使用的只有总内存的75% ResourceManager&emsp;&emsp;这是一个全局的资源管理器负责整个系统的资源管理和分配，包括处理客户端请求、启动/监控APP master、监控nodemanager、资源的分配与调度。它主要由两个组件构成：ApplicationsManager，ResourceScheduler ApplicationsManager&emsp;&emsp;应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等,跟踪分给Container的进度、状态也是其职责。&emsp;&emsp;我们客户端提交程序到ResourceManager进程的时候实际上是提交到了ApplicationsManager组件里 ResourceScheduler&emsp;&emsp;调度器它是“纯调度器”不从事任何与具体应用程序相关的工作，仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念“资源容器”（Resource Container，简称Container）表示。Container是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。 NodeManager&emsp;&emsp;Nodemanager它管理Hadoop集群中单个计算节点,他需要与应用程序的ApplicationMaster(用户提交作业的老大)和集群资源管理器RM(调度框架的老大)交互 从ApplicationMaster上接收到相关Container的命令(启动，停止Container)并执行 Nodemanager定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态（cpu和内存等资源） ApplicationMaster（AM）&emsp;&emsp;管理YARN内运行的应用程序的每个实例,用户提交的每个应用程序均包含一个AM 向RS调度器申请资源 与nm协同工作完成task的执行和监控 监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务 Container&emsp;&emsp;container是虚拟的概念，是yarn中的资源抽象,它运行在nodemanager进程所在的机器上的，它封装了某个节点上的多维度资源，如内存、CPU vcores，磁盘，网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container表示的。YARN会为每个任务(用户提交的一个作业会被拆分为多个maptask任务和reducetask任务)分配一个Container，且该任务只能使用该Container中描述的资源。需要注意的是，Container它是一个动态资源划分单位，是根据应用程序的需求动态生成的。目前为止，YARN仅支持CPU和内存两种资源。提交作业到yarn的时候可以指定container的配置。但是如果没有指定，yarn会分配container的默认配置，如果资源不够的话，正常情况可以动态加的，会有个最大值的范围，如果超过最大值container就爆掉了会抛出oom。 Mr on yarn提交流程 1.client向rm提交应用程序（jar） 其中已经包含ApplicationMaster主程序和启动命令2.RS会返回资源提交的路径3.客户端将job资源(切片信息,xml文件,jar包)提交到该路径4.资源提交完毕后,向RM申请启动appmaster5.rm将用户的请求初始化成task放到调度队列中6.ApplicationsManager会为job分配第一个container，运行ApplicationMaster,Applicationmaster启动后向ApplicationsManager注册之后在yarn的web界面就能看到这个job的运行状态7.Applicationmaster采取轮询的方式通过【RPC】协议让ResourceScheduler，申请和领取资源（哪台nm 多个内存 多少cpu vcore）8.之后下载刚才提交的job资源到本地 1-8步 启动app master（应用的主程序），领取到资源列表 9.一旦app master拿到资源列表，就和对应的nm进程通信，要求启动的任务maptask计算代码10.NM为任务设置好运行环境(container容器 包含jar包等资源)11.MrAppMaster将任务启动命令写在一个脚本里，并通过该脚本启动任务maptask。各个任务task通过【rpc】协议向app master汇报自己的进度和状态，以此让app master随时掌握task的运行状态。当task运行失败，会在另外一个container容器里面重启任务。12.maptask运行完成后，MrAppMaster会向RM申请container运行reducetask。13.reducetask会从map处拿取相应分区数据,然后执行14.当所有task运行完成后，app master向 apps manager申请注销和关闭作业 这时在yarn web页面看任务是不是完成的，完成后的状态是成功还是失败的 运行任务，直到任务完成。 简单总结就是: 启动app master（应用的主程序），领取到资源列表 ，运行任务，直到任务完成。 Yarn调优 关于Yarn的调优，就是调Container容器，因为task任务就是放到container中执行的。 生产上的部署一般遵循存储计算一体，即计算时发现本节点有数据不需要去其他节点拉取数据，节省网络io，这种一般叫做数据本地化。所以生产上的机器一般有DN就会有NM，这俩部署在一起 案例:假设公司买了台服务器，128G内存，16核 物理 core。&emsp;&emsp;1.1 系统装完 消耗1G内存&emsp;&emsp;1.2 系统预留20%的内存（①防止触发oom-killer机制 ②Linux系统防止夯住 ③给未来部署的组件预留部分空间），这个不是一成不变的，看自己机器的内存基数（64G还是128G）&emsp;&emsp;1.3 假设该台机器只部署DN（存储数据）、NM（计算）节点 生产上DataNode应该给多少G内存？&emsp;&emsp;生产上设了2G，这是很多了，其实只用了900多M，业务高峰期也没有超过1G，这根据每秒钟数据量来的。官网默认参数1000M 生产上NodeManager应该给多少G内存？&emsp;&emsp;生产上设了4G，可以看上面nm的作用 128G*80%=102G, 102G-2G-4G=96G , 这96G全部用来设计给真正干活的Container容器用。 调整Container容器的参数如果仅仅考虑内存,配置和调整参数:yarn.nodemanager.resource.memory-mb&emsp;&emsp;所有container可用的总的内存 默认是8G 调成96Gyarn.scheduler.minimum-allocation-mb&emsp;&emsp;单个Container容器最小的内存大小，极限情况下有96个Container每个1G 默认是1Gyarn.scheduler.maximum-allocation-mb&emsp;&emsp;单个Container容器最大的内存大小，极限情况下有1个Container大小是96G 默认是8G 调成96G Container执行任务时候，如果发现内存不够了，可以动态增加资源（cdh yarn配置的默认1G递增（目前apache没有该配置），但是一般不调整），一直加到设置的最大参数。 如果仅仅考虑cpu vcore （container容器虚拟core）(生产上默认不考虑预留core) cpu vcore介绍&emsp;&emsp;vcore是yarn自己设计引入的，设计初衷是考虑不同机器的cpu的物理core性能不一样，每个cpu计算的能力不一样。比如第一台机器物理cpu是第二台机器物理cpu的2倍，这时通过设置第一个物理cpu的虚拟core来弥补这个差异 第一台机器强悍 pcore（物理core）：vcore（虚拟core）=1:2第二台机器弱些 pcore（物理core）：vcore（虚拟core）=1:1 现在生产上基本上不可能设置谁强悍谁不强悍。统一设置为pcore（物理core）：vcore（虚拟core）=1:2为什么要设置1:2呢？因为计算的时候container容器需要vcore，并发任务是靠vcore。当然也不能调太大，这需要经验值，由大公司得来的 具体配置yarn.nodemanager.resource.cpu-vcores&emsp;&emsp;container可用的总的vcore，默认是8，调整为32（假设pcore为16）yarn.scheduler.minimum-allocation-vcores&emsp;&emsp;一个container容器最小分配vcore数，默认为1，极限情况下有32个container容器yarn.scheduler.maximum-allocation-vcores&emsp;&emsp;一个container容器最大分配vcore数，默认为32，极限情况下有1个container容器yarn.nodemanager.resource.pcores-vcores-multiplier&emsp;&emsp;vcore：pcore，默认的是1，调整为2 想要让一台机器让任务计算快一点，不光光需要考虑当前任务的cpu和内存，还要考虑container的个数是多少 生产上面如何设置？clouder公司推荐，一个Container的vcore最好不要超过5个，所以生产上设置单个container能申请的vcore最大为4(这个4就是参数设置的突破口)，如果每个container都选最大的vcore极限情况下只有8个Container（比如vcore是32=4*8） 整合memory + cpu,配置和调整参数:确定最大4个vocre极限时候只有8个container yarn.nodemanager.resource.memory-mb&emsp;&emsp;调成96Gyarn.scheduler.minimum-allocation-mbyarn.scheduler.maximum-allocation-mb&emsp;&emsp;单个Container的vcore越多Container的总个数就越少,Container总数目越少,单个container能申请的内存就越多,所以最大调成96/8=12G 但是spark计算时内存有些指标比较大,那么这个参数必然调大,那么这种理想化完美化的设置必然被打破,到时以memory为主 yarn.nodemanager.resource.cpu-vcores&emsp;&emsp;调整为32（假设pcore为16）yarn.scheduler.minimum-allocation-vcores&emsp;&emsp;调整为1yarn.scheduler.maximum-allocation-vcores&emsp;&emsp;调整为4yarn.nodemanager.resource.pcores-vcores-multiplier&emsp;&emsp;调整为2 yarn.nodemanager.pmem-check-enabled&emsp;&emsp;container容器超过物理内存是否杀死容器,默认是true,生产生改为falseyarn.nodemanager.vmem-check-enabled&emsp;&emsp;container容器超过虚拟内存是否杀死容器默认是false,生产生改为falseyarn.nodemanager.vmem-pmem-ratio&emsp;&emsp;2.1 就是有个虚拟内存的概念 一般不用 不要调整这个参数 思考:假如该节点还有其他组件，比如多了个HBase RS节点机器配置:256G内存 ,cpu物理core 32核三个进程:DN,NM,HBase RS=30G (内存最好不要超过32G)请问上面七个参数如何设置? JVM内存设置不要超过32G原因123456事实上jvm在内存小于32G的时候会采用一个内存对象指针压缩技术。在java中，所有的对象都分配在堆上，然后有一个指针引用它。指向这些对象的指针大小通常是CPU的字长的大小，不是32bit就是64bit，这取决于你的处理器，指针指向了你的值的精确位置。对于32位系统，你的内存最大可使用4G(32位表示的最大的数字(2^32)代表的内存大小是4G)。对于64系统可以使用更大的内存。但是64位的指针意味着更大的浪费，因为你的指针本身大了。浪费内存不算，更糟糕的是，更大的指针在主内存和缓存器（例如LLC, L1等）之间移动数据的时候，会占用更多的带宽。Java 使用一个叫内存指针压缩的技术来解决这个问题。它的指针不再表示对象在内存中的精确位置，而是表示偏移量。这意味着32位的指针可以引用40亿个对象，而不是40亿个字节。最终，也就是说堆内存长到32G的物理内存，也可以用32bit的指针表示。一旦你越过那个神奇的30-32G的边界，指针就会切回普通对象的指针，每个对象的指针都变长了，就会使用更多的CPU内存带宽，也就是说你实际上失去了更多的内存。事实上当内存到达40-50GB的时候，有效内存才相当于使用内存对象指针压缩技术时候的32G内存。这段描述的意思就是说：即便你有足够的内存，也尽量不要超过32G，因为它浪费了内存，降低了CPU的性能，还要让GC应对大内存。 yarn命令：yarn –help:查看帮助得到部分命令如下&emsp;&emsp;yarn logs applicationid 但是一般都用web界面看日志&emsp;&emsp;yarn application + 回车&emsp;&emsp;&emsp;&emsp;yarn application -kill applicationid&emsp;&emsp;&emsp;&emsp;yarn application -list Yarn调度器在Yarn中有三种调度器可以选择:FIFO Scheduler,Capacity Scheduler,FairScheduler FIFO Scheduler&emsp;&emsp;把应用提交的顺序排成一个队列,这是一个先进先出的队列,在进行资源分配的时候,先给队列中最头上的应用进行分配资源,如果第一个app需要的资源被满足了，如果还剩下了资源并且满足第二个app需要的资源，那么就为第二个app分配资源,以次类推。所以它不适用于共享集群,大的应用可能会占用所有集群资源,这就导致其他应用被阻塞。 Capacity Scheduler&emsp;&emsp;CapacityScheduler允许将整个集群的资源分成多个队列，每个队列还可以进一步划分成层次结构（Hierarchical Queues），从而允许组织内部的不同用户组的使用。每个队列内部，按照FIFO的方式调度Applications。当某个队列的资源空闲时，可以将它的剩余资源共享给其他队列。 FairScheduler&emsp;&emsp;在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。需要注意的是，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器既得到了高的资源利用率又能保证小任务及时完成。 Apache 默认的资源调度器是容量调度器；CDH 默认的资源调度器是公平调度器。 结束","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://blog.utone.xyz/tags/hadoop/"},{"name":"Yarn","slug":"Yarn","permalink":"https://blog.utone.xyz/tags/Yarn/"}]},{"title":"MapReduce","slug":"hadoop/MapReduce","date":"2016-06-23T04:00:00.000Z","updated":"2020-06-17T14:27:53.666Z","comments":true,"path":"2016/06/23/hadoop/MapReduce/","link":"","permalink":"https://blog.utone.xyz/2016/06/23/hadoop/MapReduce/","excerpt":"","text":"定义&emsp;&emsp;MapReduce是一个分布式计算框架，开发复杂累赘所以除了老项目现在生产上基本不用了，生产中用hive，spark，fink代替。但是MapReduce计算框架的设计理念，成就了很多计算框架，所以可以认为是大数据计算框架的鼻祖。&emsp;&emsp;在运行一个mapreduce计算任务时候，任务过程被分为两个阶段：map阶段和reduce阶段，map是负责数据的过滤和分发，reduce负责数据的计算归并 大数据计算主要靠两样:内存+cpu vcore，这个可以由yarn提供 &emsp;&emsp;一个完整的MapReduce程序在分布式运行时有三类实例进程: MrAppMaster:负责整个程序的过程调度及状态协调 MapTask:负责map阶段的整个数据处理流程 ReduceTask:负责reduce阶段的整个数据处理流程 MapReduce详细工作流程 MapTask详细工作流程开多少个maptask完全由切片个数决定,而不是由块决定。一个切片对应一个maptask(但要注意,一个文件如果小于128M也会被切成一片的,所以切片个数也受文件数影响)。默认切片大小等于块大小等于128M。 Read阶段：Map Task通过RecordReader，从输入InputSplit中解析出一个个key/value。 Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。 map()方法后就进入了shuffle阶段 Shuffle&emsp;&emsp;shuffle具体是指Map()方法之后Reduce()方法之前这段处理过程叫 Shuffle。这需要把不同机器相同key的数据聚集在一个reduce上，这其中涉及到了大量的磁盘操作和网络io，严重影响计算性能。&emsp;&emsp;所以业界有句话，能不shuffle的就不要shuffle，shuffle会拉长计算的时间。 shuffle具体执行过程:&emsp;&emsp;每个map任务都有一个环形换冲区用于存储任务输出，在默认情况下缓冲区大小是100m，此值可以通过io.sort.mb属性来调整。一旦缓冲区达到阈值(io.sort.spill.percent，默认是0.80)，一个后台线程就会把内容溢出到磁盘（在溢出到磁盘的过程中，map输出会继续写到剩余20%的缓存区中，但如果在此期间缓存区被填满了，map会被阻塞直到写磁盘过程完成。溢出过程将缓存数据写到mapred.local.dir属性指定的目录中）。在写磁盘之前，线程会首先将数据进行分区。在每个分区中，后台线程按照键进行排序(排序方式为快速排序,如果我们定义了combiner函数，排序后接着运行combiner在本地节点内存中将每个Map任务输出的中间结果按键做本地聚合(如果设置了的话)，可以减少传递给Reducer的数据量。)，每次环形缓冲区达到溢出阈值，就会新建一个溢出文件，也就是说在做map输出有几次spill就会产生多少个溢出文件，因此在map任务写完其最后一个输出记录后，会有很多溢出文件。之后溢出文件会被合并成一个已分区且已排序的输出文件。配置属性io.sort.factor控制着一次最多能合并多少流，默认值是10。在将map输出写到磁盘上过程中对其进行压缩是很好的主意，因为这样写磁盘更快，节约磁盘空间，并且减少传给reduce的数据量，但是在reduce端是需要解压缩的。在默认情况下输出是不压缩的，但只要将mapred.compress.map.output设置为true ， 就可以轻松启用此功能。使用压缩库由mapred.map.output.compression.codec指定。reduce通过http得到输出文件的分区，因为每个map的完成时间各不相同，因此只要有一个任务完成，reduce任务就开始复制其输出.这就是reduce任务的复制阶段。reduce任务有少量的复制线程，因此能够并行的取得map输出。默认值是5个线程，可以通过参数mapred.reduce.parallel.copies属性来改变。如果map输出相当小，会被复制到reduce任务的jvm内存中(缓冲区大小由mapred.job.shuffle.input.buffer.percent属性控制，指定用于此用途的堆空间占比)，否则map输出复制到磁盘。一旦内存缓冲区达到阈值大小(mapred.job.shuffle.merger.percent决定)或达到map输出阈值(mapred.inmen.merge.threhold控制)，则合并后溢出写到磁盘。注意为了合并压缩后的map输出必须再内存中解压缩。复制完所有的map数据后，reduce阶段进行归并排序(由于各个MapTask已经实现对自己的处理结果进行了局部排序)。 shuffle优化 ReduceTask详细工作流程 之前是shuffle操作 Reduce阶段：执行用户自定的reduce()函数将计算结果写到HDFS上。","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://blog.utone.xyz/tags/hadoop/"},{"name":"MapReduce","slug":"MapReduce","permalink":"https://blog.utone.xyz/tags/MapReduce/"}]},{"title":"HDFS","slug":"hadoop/hdfs","date":"2016-06-01T04:00:00.000Z","updated":"2020-06-19T03:44:42.234Z","comments":true,"path":"2016/06/01/hadoop/hdfs/","link":"","permalink":"https://blog.utone.xyz/2016/06/01/hadoop/hdfs/","excerpt":"","text":"大数据:存储（hdfs），计算（MR，hive，spark，flink），资源和作业调度（yarn）。其中存储是最重要的，一切都是建立在存储的数据上的。 HDFS组成&emsp;&emsp;有三个java进程，分别是NameNode（名称节点），DataNode（数据节点），SecondaryNamenode（第二名称节点，生产中不用）HDFS是主从架构（老大不干活只是负责管理，干活的都是小弟） NameNode&emsp;&emsp;NameNode主要维护 a.文件的名称 b.文件的路径 c.文件的属性，权限，创建时间，副本数 d.一个文件包括其副本被对应切割哪些数据块，这些数据块对应被分布在哪些节点上，blockmap块映射:当然NN节点是不会持久化存储blockmap这种映射关系，是通过集群的启动和运行时DN定期发送blockreport给nn，然后nn就在内存中动态维护这种映射关系(可能实时对数据写入，如果元数据信息持久化维护到磁盘是不利于动态更新的) &emsp;&emsp;NameNode被格式化后，将在参数【core-site.xml文件里定义hadoop.tmp.dir】/dfs/name/current/目录中生成如下文件：fsimage，editlog，seen_txid 下图是NN的编辑日志和镜像文件： 镜像文件（fsimage） HDFS文件系统元数据的一个永久性检查点，其中包含HDFS文件系统的所有目录和文件idnode的序列化信息。Fsimage只会保留最新的和第二新的，旧的全部删除 编辑日志（edits） 存放HDFS文件系统的所有更新操作的路径，一个小时一次滚动，每次都会生成一个已经写完的文件，文件系统客户端执行的所有写操作首先会被记录到edits文件中。edits合并后也不会删除 seen_txid 该文件保存的是一个数字，就是最后一个edits_的数字 nn工作流程&emsp;&emsp;元数据都放在内存中，但是怎么持久化呢，不能一关机元数据就都没了 &emsp;&emsp;元数据在内存中很大，持久化我们想到了Redis面临大块内存持久化策略：RDB和AOF。 123Redis HadoopRDB FsimageAOF edits.log RDB：持久化过程时间更长，不能每次操作都生成一次RDB需要隔一段时间或隔一定操作量生成一次RDB。好处是加载快–&gt;加载快,生成慢,安全性略低 AOD：记录了每条操作，占用的空间会更大。好处是每步操作都会记录，更安全。–&gt;加载慢,生成快,安全性高 &emsp;&emsp;Hadoop也是有两种类型解决方案，HDFS对数据一致性和安全性非常高，如果持久化采用类AOF策略：edits.log这个文件实时记录了hadoop所有的元数据变动，确保数据安全性，缺点就是加载慢，如果我重启HDFS，会读取非常慢。如果持久化采用类RDB策略：Fsimage这个生成很慢，在写入过程中由于是做整个内存镜像所以只能对外提供读服务，不能提供写服务，因为正在持久化内存数据那么这个数据不能动。企业中Hadoop读写非常频繁，不能出现空档期 &emsp;&emsp;所以hadoop自己持久化edits.log但是还希望有Fsimage。所以2nn出来了，2nn就是帮助nn把edits.log合并成Fsimage 12345678910111213141）第一阶段：namenode启动（1）第一次启动namenode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。（2）客户端对元数据进行增删改的请求（3）namenode记录操作日志，更新滚动日志。（4）namenode在内存中对数据进行增删改查2）第二阶段：Secondary NameNode工作 （1）Secondary NameNode询问namenode是否需要checkpoint。直接带回namenode是否检查结果。 （2）Secondary NameNode请求执行checkpoint。 （3）namenode滚动正在写的edits日志 （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。 （6）生成新的镜像文件fsimage.chkpoint （7）拷贝fsimage.chkpoint到namenode （8）namenode将fsimage.chkpoint重新命名成fsimage DataNode&emsp;&emsp;存储数据块和数据块的校验和(校验和数据块第一次进入hdfs时候，会给这个块算一个md5的值，后续集群会定期通过初始存储的md5值和现在存储这个块的md5值比较来查看块是否损坏，损坏后集群可以修复)&emsp;&emsp;作用:&emsp;&emsp;a.每隔3秒发送心跳给NN，告诉NN自己还活着 调整参数:dfs.heartbeat.interval 默认是3s&emsp;&emsp;b.dfs.blockreport.intervalMsec(datanode向namenode报告块信息的时间间隔即blockreport) 默认是21600000ms=6小时 ， dfs.datanode.directoryscan.interval(datanode进行内存和磁盘数据集块校验，更新内存中的信息和磁盘中信息的不一致情况) 21600s=6小时 (总结，在六小时内把差异化目录扫描，然后把块报告发送出去，这两个缺一不可..未来调优时候这两个参数值同时调调成一样..这两个参数控制隔多久hdfs检测并且自我修复) &emsp;&emsp;生产中偶尔监控到块损坏，可以不用自己修复，集群会自己修复 (生产案例，丢失了将近10000块，恢复时间一天多)&emsp;&emsp;上传文件切的块在linux上存储的位置是:/home/hadoop/tmp/dfs/data1(有多个data1，data2，…)/current/BP-1240559480-192.168.1.102-1529720722096/current/finalized/subdir0/subdir0/(有多个subdir0，subdir1，…) 块 block块是物理上的概念，切片是逻辑上的概念，不要混为一谈 块和切片的区别块：指的是文件在HDFS上的存储，将要存储的数据分成一块又一块，然后放在HDFS上存储； 切片：指的是MapReduce阶段的FileInputFormat阶段对于数据块的切片，默认的切片大小是块大小。然后每一个切片分配一个map(mapTask)处理 块大小&emsp;&emsp;HDFS中的文件在物理上是分块存储的。HDFS存储大文件是利好，存储小文件是损害自己的。即适合存储大文件，不适合存储小文件（不代表不能存储小文件），因为NN是存储元数据信息的，一亿个10KB的文件在hdfs上存储会生成三亿个block(默认副本数是3)，如果将这些文件压缩成1KW个100M的文件在hdfs上存储会生成3KW个block，3KW肯定比3亿对NN压力小，所以生产中尽量不要有小文件存在&emsp;&emsp;HDFS上的文件会被切割成块，块的大小在hdfs-site.xml文件里配置，配置参数是dfs.blocksize 默认参数:134217728 =128M（当然也可以调整为256M，这个可以进一步降低NN的压力）案例: 260m文件切割为 128m，128m，4m这三个块 副本数&emsp;&emsp;副本就是为了保证数据存储的可靠性防止单点故障造成数据丢失，给每个块都做备份分布式存储到不同的机器上。如副本数设为3，那么这个块一共存储3份在hdfs上&emsp;&emsp;配置参数是 dfs.replication，生产上HDFS是集群的，这个参数一般设置为3 最开始hadoop默认块大小是64M，后来调整为128M，为什么？比如mv文件是260M 如果块大小是64M。260/64=4余4M，相当于划分为五个块分别为:b1(64M)，b2(64M)，b3(64M)，b4(64M)，b5(4M) 如果块大小是128M。260/128=2余4M，相当于划分为三个块分别为:b1(128M)，b2(128M)，b3(4M) 总结：无论块大小是64M还是128M，存储实际大小都是:260M*3=780M (副本数都是3) 。区别就是分成不同的块，5个块的元数据信息维护要比3个块的元数据信息维护重，会对NN造成压力 hdfs写流程网上：https://www.imbajin.com/2020-01-18-HDFS%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BClient%E5%86%99%E6%B5%81%E7%A8%8B%E4%B8%80/&emsp;&emsp;1.源码中HDFS Client调用DistributedFileSystem.create(filePath)方法，去和NN进行通信，申请上传文件！NN 会去check这个路径的文件名是否已经存在，是否有权限能够创建这个文件！如果都满足，就创建空的文件。create方法返回值是该文件输出流FSDataOutputStream&emsp;&emsp;2.之后client向nn请求上传第一个block，nn返回对应的dn比如三个副本分别是dn1，dn2，dn3。之后dn1，dn2，dn3会依次建立通道，client收到应答后，说明通道建立成功&emsp;&emsp;3.Client调用FSDataOutputStream对象的write方法，传输packet（64K一个），当dn1收到后一边本地落盘一边发给dn2，dn2也同理，dn3落盘成功后就返回一个ack packet确定包给DN2节点，当DN2节点收到这个ack packet确定包加上自己也是写完了，就返回一个ack packet确定包给第一个DN1节点，当DN1节点收到这个ack packet确定包加上自己也是写完了，将ack packet确定包返回给【FSDataOutputStream】对象，每当发送一个packet就将该packet放到ack队列中，若收到ack就说明成功了，则删除ack队列中的对应packet。当前block传输完成后, client会接着再申请一个新的block, 直到所有数据传输完成&emsp;&emsp;4.当所有的块全部写完， client调用 FSDataOutputStream对象的close方法，关闭输出流。再次调用FileSystem.complete方法，告诉nn文件写成功！ 注:伪分布式 1台dn 副本数参数必然是设置1，dn挂了，肯定不能写入 生产分布式 3台dn 副本数参数设置3，dn挂了，肯定不能写入 生产分布式 &gt;3台dn 副本数参数设置3，dn挂了，能写入 总结就是:存活的alive dn节点数&gt;=副本数 就能写成功 hdfs读流程&emsp;&emsp;1.Client调用FileSystem的open(filePath)，与NN进行RPC通信，namenode检查文件是否存在，如果存在则client会请求下载第一个block，nn返回对应的dn列表&emsp;&emsp;2.Client调用FSDataInputStream对象的read方法去与第一个块的最近的DN进行读取，读取完成后，会继续请求读取第二个块以此类推。&emsp;&emsp;3.Client调用FSDataInputStream对象close方法，关闭输入流。 hdfs副本放置策略 （面试和生产都需要）生产上读写操作 尽量选择 某个DN节点，因为这样第一个副本可以减少网络消耗第一个副本：放置在上传的DN节点；假如是非DN节点，就随机挑选一个磁盘不太慢，cpu不太忙的节点； 第二个副本：放置在第一个副本不同的机架上的某个DN节点。一般一个机架最多装10台机器，一般7，8台。生产上也没有配置机架的配置，不配置就认为所有的机器在一个整体的机架。 第三个副本:与第二个副本相同机架的不同节点上。 SecondaryNamenodeHA中StandBy nn会干2NN干的活,而且还会干的更好是实时备份&emsp;&emsp;SNN主要是为了解决NN的单点故障，做1小时的备份，在NN发生故障的时候可以从SNN恢复数据，虽然能够减轻单点故障带来的丢数风险，但是在生产上还是不允许使用SNN&emsp;&emsp;方法就是将nn的fsimage和editlog定期拿过来合并，备份，推送 checkPoint参数，两个参数是或的关系： dfs.namenode.checkpoint.period 3600s （做检查点（checkpoint）间隔时间） dfs.namenode.checkpoint.txns 1000000 （写数据条数，如果写到了1000000也会做checkpoint） &emsp;&emsp;如果11:00 SNN备份了，但是11:30的时候突然NN节点磁盘故障，此时拿SNN的最新的fsimage文件恢复，那么只能恢复11点之前的数据。虽然11:00-11:30的数据还在HDFS上，但是这些数据的元数据信息丢失了，那么无法使用了。 &emsp;&emsp;因为还是存在丢失部分元数据信息的可能，所以生产上不会使用SNN，是启动另一个NN进程，这个进程实时备份，实时准备替换NN，变为活动的NN。。这个架构叫做HDFS HA HDFS命令1234567891011121314151617181920[hadoop@hadoop001 hadoop]$ lltotal 132drwxr-xr-x 2 hadoop hadoop 128 Jun 3 2019 bin 可执行脚本 命令drwxr-xr-x 2 hadoop hadoop 4096 Jun 3 2019 bin-mapreduce1drwxr-xr-x 3 hadoop hadoop 4096 Jun 3 2019 clouderadrwxr-xr-x 6 hadoop hadoop 105 Jun 3 2019 etc 配置文件夹，也有的叫conf&#x2F;configdrwxr-xr-x 5 hadoop hadoop 40 Jun 3 2019 examples 案例drwxr-xr-x 3 hadoop hadoop 27 Jun 3 2019 examples-mapreduce1drwxr-xr-x 2 hadoop hadoop 101 Jun 3 2019 includedrwxr-xr-x 3 hadoop hadoop 19 Jun 3 2019 lib jar包drwxr-xr-x 3 hadoop hadoop 4096 Jun 3 2019 libexec-rw-r--r-- 1 hadoop hadoop 85063 Jun 3 2019 LICENSE.txtdrwxrwxr-x 3 hadoop hadoop 4096 May 9 22:39 logs 日志的-rw-r--r-- 1 hadoop hadoop 14978 Jun 3 2019 NOTICE.txtdrwxrwxr-x 2 hadoop hadoop 40 May 6 23:13 output-rw-r--r-- 1 hadoop hadoop 41 May 9 21:43 part-r-00000-rw-r--r-- 1 hadoop hadoop 1366 Jun 3 2019 README.txtdrwxr-xr-x 3 hadoop hadoop 4096 Jun 3 2019 sbin 启动 停止 重启脚本drwxr-xr-x 4 hadoop hadoop 29 Jun 3 2019 sharedrwxr-xr-x 18 hadoop hadoop 4096 Jun 3 2019 src 早期的命令都是hadoop fs现在都是hdfs dfs 。看hadoop脚本都是调用的的同一个shell，所以用哪个都一样，脚本如下： 1234567hadoop fs -ls &#x2F; if [ &quot;$COMMAND&quot; &#x3D; &quot;fs&quot; ] ; then CLASS&#x3D;org.apache.hadoop.fs.FsShellhdfs dfs -ls &#x2F; elif [ &quot;$COMMAND&quot; &#x3D; &quot;dfs&quot; ] ; then CLASS&#x3D;org.apache.hadoop.fs.FsShell hadoop常用命令:1.显示目录信息 12345678[hadoop@hadoop102 hadoop]$ hadoop fs -ls &#x2F;[hadoop@hadoop102 hadoop]$ hadoop fs -ls -R &#x2F;hadoop fs -ls 其实访问是这个路径 &#x2F;user&#x2F;hadoop(linux当前用户)&#x2F;hadoop fs -ls &#x2F; 其实直接 &#x2F; 是因为默认加上了core-site.xml参数里的fs.default参数对应的值hdfs:&#x2F;&#x2F;hadoop102:9000hadoop fs -ls hdfs:&#x2F;&#x2F;hadoop102:9000&#x2F; 2.在hdfs上创建目录 123[hadoop@hadoop102 hadoop]$ hadoop fs -mkdir &#x2F;shuihu 创建多级文件夹 [hadoop@hadoop102 hadoop]$ hadoop fs -mkdir -p &#x2F;shuihu&#x2F;liangshan&#x2F;sj 3.从本地剪切到hdfs 1[hadoop@hadoop102 hadoop]$ hadoop fs -moveFromLocal dc.txt &#x2F;shuihu&#x2F;liangshan&#x2F;sj 4.从本地文件系统中拷贝文件到hdfs路径去 1[hadoop@hadoop102 hadoop]$ hadoop fs -put NOTICE.txt &#x2F; 5.从hdfs拷贝到本地(此时的本地是linux) 1[hadoop@hadoop102 hadoop]$ hadoop fs -get &#x2F;shuihu&#x2F;liangshan&#x2F;sj&#x2F;dc.txt .&#x2F; 6.从hdfs的一个路径拷贝到hdfs的另一个路径 1[hadoop@hadoop102 hadoop]$ hadoop fs -cp &#x2F;shuihu&#x2F;liangshan&#x2F;sj&#x2F;dc.txt &#x2F;shuihu&#x2F;liangshan&#x2F; 7.在hdfs上移动文件(相当于剪切) (尽量少用，可以先复制然后校验复制的是否完整后，在回来删除数据，防止剪切时候出问题数据丢失) 1[hadoop@hadoop102 hadoop]$ hadoop fs -mv &#x2F;shuihu&#x2F;liangshan&#x2F;dc.txt &#x2F; 8.统计hdfs可用空间大小 1[hadoop@hadoop102 hadoop]$ hadoop fs -df -h &#x2F; 9.删除文件或文件夹 12345678910111213141516-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...][hadoop@hadoop102 hadoop]$ hadoop fs -rm &#x2F;dc.txt [hadoop@hadoop102 hadoop]$ hadoop fs -rm -r &#x2F;my生产上: 1.开启回收站！+回收站的有效期至少7天(具体几天自己配置，建议至少7天)！！ (配置文件是core-site.xml中) 2.慎用-skipTrash，否则数据直接删除不会进入回收站！！[hadoop@hadoop001 hadoop]$ hdfs dfs -rm &#x2F;1.log20&#x2F;05&#x2F;12 21:45:27 INFO fs.TrashPolicyDefault: Moved: &#39;hdfs:&#x2F;&#x2F;hadoop001:9000&#x2F;1.log&#39; to trash at: hdfs:&#x2F;&#x2F;hadoop001:9000&#x2F;user&#x2F;hadoop&#x2F;.Trash&#x2F;Current&#x2F;1.log注:hdfs:&#x2F;&#x2F;hadoop001:9000&#x2F;user&#x2F;hadoop&#x2F;.Trash&#x2F;Current&#x2F;1.log 这个文件hdfs将会保留7天(参数配置)，7天后会被删除[hadoop@hadoop001 hadoop]$ hdfs dfs -rm -skipTrash &#x2F;2.logDeleted &#x2F;2.log 10.-chown -chmod跟linux一样 &emsp;&emsp;-get 等同于-copyToLocal (下载)&emsp;&emsp;-put 等同于-copyFromLocal (上传) 11.检查hadoop支持的压缩: 12345678910[hadoop@ruozedata001 hadoop]$ hadoop checknative20&#x2F;05&#x2F;12 21:53:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableNative library checking:hadoop: false zlib: false snappy: false lz4: false bzip2: false openssl: false 20&#x2F;05&#x2F;12 21:53:26 INFO util.ExitUtil: Exiting with status 1 12.得到hdfs某一路径下大小: 1234该路径下各个文件夹分别大小:hadoop fs -du -h &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;ori_partitioned该路径下所有文件夹总共大小:hadoop fs -du -s &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;ori_partitioned 13.得到hdfs某一路径下，文件个数(加上了查询的根路径)和大小 1hadoop fsck &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;ori_partitioned hdfs常用命令:1.hdfs dfsadmin 打开hdfs管理员客户端 hdfs dfsadmin -report 可以看系统健康状况，好处是我可以写shell脚本定时采集这个数据，将数据传输到监控平台进行数据上报达到监控 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465Configured Capacity: 62608195584 (58.31 GB)Present Capacity: 9957312247 (9.27 GB)DFS Remaining: 6733078528 (6.27 GB)DFS Used: 3224233719 (3.00 GB)DFS Used%: 32.38%Under replicated blocks: 153Blocks with corrupt replicas: 0Missing blocks: 0Missing blocks (with replication factor 1): 0-------------------------------------------------Live datanodes (3):Name: 192.168.1.103:50010 (hadoop103)Hostname: hadoop103Decommission Status : NormalConfigured Capacity: 20869398528 (19.44 GB)DFS Used: 1164574636 (1.08 GB)Non DFS Used: 17228857428 (16.05 GB)DFS Remaining: 2475966464 (2.31 GB)DFS Used%: 5.58%DFS Remaining%: 11.86%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Thu May 28 19:24:37 CST 2020Name: 192.168.1.104:50010 (hadoop104)Hostname: hadoop104Decommission Status : NormalConfigured Capacity: 20869398528 (19.44 GB)DFS Used: 1027734441 (980.12 MB)Non DFS Used: 17687430231 (16.47 GB)DFS Remaining: 2154233856 (2.01 GB)DFS Used%: 4.92%DFS Remaining%: 10.32%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Thu May 28 19:24:38 CST 2020Name: 192.168.1.102:50010 (hadoop102)Hostname: hadoop102Decommission Status : NormalConfigured Capacity: 20869398528 (19.44 GB)DFS Used: 1031924642 (984.12 MB)Non DFS Used: 17734595678 (16.52 GB)DFS Remaining: 2102878208 (1.96 GB)DFS Used%: 4.94%DFS Remaining%: 10.08%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Thu May 28 19:24:37 CST 2020 hdfs dfsadmin [-safemode &lt;enter | leave | get | wait&gt;] 安全模式，集群进入安全模式不能写数据，但是能读数据。错误:Name node is in safe mode。手动关闭安全模式：hdfs dfsadmin -safemode leave 什么时候会进入安全模式？ （故障场景）hdfs发生了故障，检查nn的log日志发现进入了安全模式，看什么原因进入的安全模式，之后尝试手动离开安全模式。 （业务场景）集群维护时候，让上游数据截断，并且让nn进入安全模式，可以读不能写。这是双重保障。尽量不要关闭集群，不然可能再次启动就启动不起来了 HDFS数据不均衡的两个场景各个DN节点的数据平衡（注意这是集群数据不均衡而不是单个DN的）&emsp;&emsp; 比如集群有三个节点分别是dn1，dn2，dn3并且硬盘大小都相同。其中dn1使用了硬盘的90%，dn2使用了硬盘的60%，dn3使用了硬盘的80%。&emsp;&emsp; hdfs提供了一个脚本，是/home/hadoop/app/hadoop/sbin目录下的start-balancer.sh 12[hadoop@hadoop001 sbin]$ .&#x2F;start-balancer.sh -threshold 10.0starting balancer， logging to &#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop&#x2F;logs&#x2F;hadoop-hadoop(用户)-balancer-hadoop001.out 参数threshold(阈值)含义: 每个节点的磁盘使用率 - 平均的磁盘使用率 &lt; threshold%&emsp;&emsp; 例:90+60+80=230/3=76%&emsp;&emsp; &emsp;&emsp; 90%-76%=14% &gt; 10% 这里面数据要移出来部分&emsp;&emsp; &emsp;&emsp; 60%-76%=-16% &lt; 10% 其他节点的数据可以转移过来&emsp;&emsp; &emsp;&emsp; 80%-76%=4% &lt; 10% 其他节点的数据可以转移过来一共多14%+4%=18% ， 可以将这18%都转移到60%的这台机器上数据均衡后 使用率分别为(假设每个DN节点的存储空间都一样，如果不一样还需要注意传过来的14%的数据，过来后是否会撑爆掉机器) 76%(90-14)，78%(60+18)，76%(80-4) 76-76=0 &lt; 10 78-76=2 &lt; 10 76-76=0 &lt; 10均满足要求 生产上从现在开始 ./start-balancer.sh -threshold 10.0 放到业务低谷比如凌晨，每天定时去做平衡操作。如果平常不做，等最后再做平衡会有大量数据操作会造成大量的硬盘IO，网络io可能让服务器资源卡住，会影响业务 调整平衡的网络带宽 ，hdfs-site.xml文件dfs.datanode.balance.bandwidthPerSec 默认参数10m–&gt;调整为50m 单个DN的多块磁盘的数据均衡为什么要用多块物理磁盘? 1多个磁盘的io是叠加的，比如每块磁盘io每秒能写10M，那么我现在有3块磁盘，则我1s能写30M数据 &emsp;&emsp; 在投产前规划单个DN机器，配置多个磁盘路径，在公司买机器的时候，一定要说清楚，不要等后期再来做数据迁移的这些事情，磁盘也不是很贵(生产上性价比最高的是西部数据或者希捷2.5英寸，10000转/分，2T，因为超过2T是很贵的)，我们前期就先规划两年到三年的存储，不要一块磁盘一块磁盘的加。 配置参数: 1234&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt; &lt;value&gt;&#x2F;data01，&#x2F;data02，&#x2F;data03&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; &emsp;&emsp; 多块磁盘在写的时候会自己均衡，基本不会出现很大的偏差，所以如果不新加磁盘的话半个月均衡一次就可以了。但是假如目前该DN磁盘容量不够了，需要添加新的磁盘这时候就要将该DN上的磁盘做数据均衡想做单个DN的磁盘数据均衡需要打开配置参数，配置文件hdfs-site.xml:查看hadoop官网Documentation可以看到apache版本的在3.X以上才加入这个参数，但是大部分小伙伴公司是2.x所以这个特性用不了！！！&emsp;&emsp; 但是cdh版本的hadoop-2.6.0-cdh5.16.2有此功能，虽然hadoop版本是hadoop-2.6.0，但是CDH会把hadoop的新特性的地方加载到自己的本地，但是原有的版本号不会变化，相当于打了很多补丁，所以支持dfs.disk.balance&emsp;&emsp; CDH网站：http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2/ 1234&lt;property&gt; &lt;name&gt;dfs.disk.balancer.enabled&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 之后分别执行命令： 12345678910hdfs diskbalancer -plan hadoop102(当前机器名) 如果不需要均衡会提示:20&#x2F;05&#x2F;12 23:25:48 INFO command.Command: No plan generated. DiskBalancing not needed for node: hadoop102 threshold used: 10.0如果需要均衡会生成文件: hadoop102.plan.json文件执行这个文件: hdfs diskbalancer -execute hadoop102.plan.json 查询: hdfs diskbalancer -query hadoop102 有10块磁盘(比如每个磁盘2T)那么一共有20T，不做raid(运维的，磁盘高可靠但是会降低存储空间，大数据是不做raid的，因为有副本机制，丢了也能自动恢复)磁盘的配置都是linux运维做的 场景：第一个月上了一个磁盘已经使用480G，第二个月新增一个磁盘2T，有两种方法，把第一个磁盘的数据直接mv到第二个磁盘然后在第一个磁盘原先存放数据处做软连接指向第二个磁盘新移动数据的位置。但是如果新加的磁盘也是500G移动过来肯定是不行的。所以第二种方法就是多个磁盘做数据均衡 HDFS上小文件的处理小文件影响 1 个文件块，占用 namenode 内存大概 150 字节。一个小文件，即使不达到128M也会存储单独一块。所以如果小文件过多会对nn内存造成很大压力。 影响计算引擎的任务数量，比如每个小的文件都会生成一个Map任务 解决方法方法一&emsp;&emsp;采用 har 归档方式，将小文件归档 123456789101112归档： bin&#x2F;hadoop archive -archiveName myhar.har -p &#x2F;user&#x2F;atguigu &#x2F;user&#x2F;my 归档 归档名称 要归档哪一个文件 归档后放到哪查看归档： hadoop fs -lsr har:&#x2F;&#x2F;&#x2F;input.har解归档文件 hadoop fs -cp har:&#x2F;&#x2F;&#x2F; user&#x2F;my&#x2F;myhar.har &#x2F;* &#x2F;user&#x2F;atguigu &emsp;&emsp;注意打成har 文件可以大大降低namenode 的内存压力。但对于MapReduce 来说起不到任何作用，因为har文件就相当一个目录，仍然不能将小文件合并到一个split中去，一个小文件一个split ，仍然是低效的 方法二&emsp;&emsp;采用ConbinFileInputFormat来作为输入 , 它能减少切片个数比如把三个小文件化为一个切片 , 最终就是减少了maptask的个数。 方法三&emsp;&emsp;有小文件场景开启 JVM 重用；如果没有小文件，不要开启 JVM 重用，因为会一直占用使用到的 task 卡槽，直到任务完成才释放。&emsp;&emsp;JVM 重用可以使得 JVM 实例在同一个 job 中重新使用 N 次，N 的值可以在 Hadoop 的mapred-site.xml 文件中进行配置 12345&lt;property&gt; &lt;name&gt;mapreduce.job.jvm.numtasks&lt;&#x2F;name&gt; &lt;value&gt;10&lt;&#x2F;value&gt; &lt;description&gt;How many tasks to run per jvm,if set to - 1 ,there is no limit&lt;&#x2F;description&gt;&lt;&#x2F;property&gt; 方法四&emsp;&emsp;如果数据已经在hdfs上了，就是定时去合并不常用的文件。（因为数仓一般都是分区的，如果合并当天分区的数据可能业务正在读取，此时合并可能会读取数据失败。所以定时的，在业务低谷时候合并冷数据。生产上合并7(或15或30)天之前的一天数据，如今天20号那么合并14号当天的小文件） &emsp;&emsp;经验值：10M以下的文件认为小文件，一般将小文件合并到120M文件并不会完全合并到块大小（快大小默认128M），因为如果在程序合并设置为128M可能真正合并的时候就超过了128M（因为要保证文件内容完整性，可能最后一个文件合并后会超过128M）比如合并到129M则此时又会分成两块（128M，1M）其中1M又是小文件了 结束了","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://blog.utone.xyz/tags/hadoop/"},{"name":"hdfs","slug":"hdfs","permalink":"https://blog.utone.xyz/tags/hdfs/"}]},{"title":"hadoop介绍","slug":"hadoop/hadoop","date":"2016-05-21T04:00:00.000Z","updated":"2020-06-17T13:52:18.873Z","comments":true,"path":"2016/05/21/hadoop/hadoop/","link":"","permalink":"https://blog.utone.xyz/2016/05/21/hadoop/hadoop/","excerpt":"","text":"hadoop定义hadoop 广义:以Apache hadoop软件为主的生态圈 也包含了hive sqoop hbase kafka spark flink等 狭义:就是Apache hadoop软件包括:hdfs(存储) mapreduce(计算) yarn(资源和作业调度) 大数据平台: 存储是第一位因为如果数据丢了或者不准确，计算再厉害指标都会不准确，要考虑如果存储挂了在其他地方还能不能找到原始数据，不要数据一存储到hdfs就把原始数据删除了，至少近期的要压缩后保存一阵，或者冷数据迁移走放到阿里云的oss存储上 官网: 大数据组件大多数都是Apach的&emsp;&emsp;hadoop.apache.org&emsp;&emsp;hadoop.hive.org&emsp;&emsp;… 2020:生产上至今企业还是以CDH5.x为主(cloudera公司)可以傻瓜式安装 . 在6.3.3版本后采取了付费模式，这点不用担心目前生产上还可以是CDH5.x够用了，求稳对于CDH不用升级，升级是升级里面的组件hadoop和spark，这个很简单升级 hadoop-2.6.0-cdh5.16.2.tar.gz (CDH版本已经很新了为什么hadoop版本还是2.6.0，因为CDH公司在发布版本后已经打了很多补丁，它的2.6.0完全可以媲美apache hadoop的2.9) 生产注意点:https://cwiki.apache.org/confluence/display/HADOOP2/HadoopJavaVersionshttps://docs.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#pcm_jdk (标注了cdh推荐的jdk版本..) hadoop安装简单的集群搭建过程 JDK 安装 配置 SSH 免密登录 配置 hadoop 核心文件 格式化 namenode hadoop核心文件core-site.xml1234567891011&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt; &lt;value&gt;hdfs:&#x2F;&#x2F;hadoop102:9000&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt; &lt;value&gt;&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;data&#x2F;tmp&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; hdfs-site.xml12 yarn-site.xml1234567891011 &lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt; &lt;value&gt;hdfs:&#x2F;&#x2F;hadoop102:9000&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt; &lt;value&gt;&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;data&#x2F;tmp&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; mapred-site.xml12345 &lt;!-- 指定mr运行在yarn上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt; &lt;value&gt;yarn&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; slaves123hadoop102hadoop103hadoop104 hadoop-env.sh123export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_144#hadoop修改pid文件的存储目录export HADOOP_PID_DIR&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F; yarn-env.sh1export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_144 mapred-env.sh1export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk1.8.0_144 jps1234567[hadoop@hadoop001 hadoop]$ jps21712 DataNode21585 NameNode23989 ResourceManager29877 Jps24094 NodeManager21871 SecondaryNameNode 根据nn的进程号查看占用端口： 12[hadoop@hadoop001 hadoop]$ netstat -nlp| grep 21585 执行后发现 50070和9000，其中50070是web界面 如果进程起不来，不管hdfs还是yarn，要先看日志是否有error，启动的时候命令行会打印出日志的地址，我们的hadoop安装目录的logs下 hadoop启动后对应的进程标识文件存储位置：/tmp/hsperfdata_所属用户之后jps是查询该文件夹下文件 12345678910[hadoop@hadoop001 tmp]$ cd hsperfdata_hadoop[hadoop@hadoop001 hsperfdata_hadoop]$ lltotal 160-rw------- 1 hadoop hadoop 32768 May 9 22:01 21585-rw------- 1 hadoop hadoop 32768 May 9 22:01 21712-rw------- 1 hadoop hadoop 32768 May 9 22:01 21871-rw------- 1 hadoop hadoop 32768 May 9 22:01 23989-rw------- 1 hadoop hadoop 32768 May 9 22:01 24094[hadoop@hadoop001 hsperfdata_hadoop]$ mv 21712 21712.bak[hadoop@hadoop001 hsperfdata_hadoop]$ 修改其中的一个进程文件后 12345678910111213[hadoop@hadoop001 hadoop]$ jps21585 NameNode23989 ResourceManager30358 Jps24094 NodeManager21871 SecondaryNameNode[hadoop@hadoop001 hadoop]$ [hadoop@hadoop001 hsperfdata_hadoop]$ ps -ef|grep DataNodehadoop 21712 1 0 May06 ? 00:03:54 &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181&#x2F;bin&#x2F;java -Dproc_datanode -Xmx1000m -Djava.net.preferIPv4Stack&#x3D;true -Dhadoop.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop-2.6.0-cdh5.16.2&#x2F;logs -Dhadoop.log.file&#x3D;hadoop.log -Dhadoop.home.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop-2.6.0-cdh5.16.2 -Dhadoop.id.str&#x3D;hadoop -Dhadoop.root.logger&#x3D;INFO，console -Djava.library.path&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop-2.6.0-cdh5.16.2&#x2F;lib&#x2F;native -Dhadoop.policy.file&#x3D;hadoop-policy.xml -Djava.net.preferIPv4Stack&#x3D;true -Djava.net.preferIPv4Stack&#x3D;true -Djava.net.preferIPv4Stack&#x3D;true -Dhadoop.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop-2.6.0-cdh5.16.2&#x2F;logs -Dhadoop.log.file&#x3D;hadoop-hadoop-datanode-hadoop001.log -Dhadoop.home.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop-2.6.0-cdh5.16.2 -Dhadoop.id.str&#x3D;hadoop -Dhadoop.root.logger&#x3D;INFO，RFA -Djava.library.path&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop-2.6.0-cdh5.16.2&#x2F;lib&#x2F;native -Dhadoop.policy.file&#x3D;hadoop-policy.xml -Djava.net.preferIPv4Stack&#x3D;true -server -Dhadoop.security.logger&#x3D;ERROR，RFAS -Dhadoop.security.logger&#x3D;ERROR，RFAS -Dhadoop.security.logger&#x3D;ERROR，RFAS -Dhadoop.security.logger&#x3D;INFO，RFAS org.apache.hadoop.hdfs.server.datanode.DataNodehadoop 30469 30251 0 22:03 pts&#x2F;1 00:00:00 grep --color&#x3D;auto DataNode[hadoop@hadoop001 hsperfdata_hadoop]$ 上述/tmp/hsperfdata_hadoop/21712文件被移走了，之后jps就不再显示datanode了，但是datanode的进程是在的。所以如果脚本使用jps进行监控，如果上述文件丢失那么jps是找不到进程的，会造成生产的误报，实际ps -ef查询这个进程是存在的但是这个文件被移除并不影响程序的停止和启动 注意如果jps后出现 进程号 – process information unavailable 此时该进程并不能断定就不存在了，jsp是存在误报情况的，需要ps -ef|grep 进程号 判断是否进程真的存在所以准确判定进程是否存在，通过 ps -ef统计.不要被jps误导 hadoop的pid文件存储位置&emsp;&emsp;默认是存储在/tmp目录，pid文件内容是进程的pid号。在进程启动后，pid向pid文件写入进程的pid号，进程关闭时会从pid文件读出pid数字，然后kill -9 pid，关闭后再删除对应进程对应的pid文件，所以文件如果不小心被删除会出现问题 &emsp;&emsp;生产中pid文件真的可以放心的丢到/tmp维护吗????&emsp;&emsp;这是不可以的，因为linux的/tmp目录会有30天的默认删除机制，如果存在超过30天文件就会被删除，我们假设rm的pid文件被删除了，如果被系统删掉了，之后我们想关闭yarn(rm，nm)执行sbin/stop-yarn.sh，nm正常关闭对应的pid文件被删除，rm就会报错显示没有对应的pid号自然对应的pid文件也不会被删除，但是实际后台这个进程是存在的。之后如果我们没有手动关闭进程并删除对应pid文件，此时启动yarn，那么yarn会智能判定是否存在进程rm，nm如果存在就不会创建对应进程，但是会创建pid文件，这个pid文件保存的pid号并不是之前没有关闭的，这个pid号实际没有对应的进程启动，所以还是紊乱的，这是恶性循环即我的rm永远不会通过我的命令来关闭 总结：生产上pid文件不要丢在/tmp目录，这会影响进程的启动和停止。如果工作中发现机器进程没有重启成功，新改的配置没有应用上，此时我们应该查看后台实际的进程号和pid文件里保存的进程号是否一致。 hadoop修改pid文件的存储目录在公司最开始没有调这个pid的，出现故障后才开始调整在hadoop-env.sh脚本export HADOOP_PID_DIR=/home/hadoop/tmp/","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://blog.utone.xyz/tags/hadoop/"}]},{"title":"java","slug":"java/java封装继承多态","date":"2015-07-28T04:00:00.000Z","updated":"2020-06-04T02:24:54.883Z","comments":true,"path":"2015/07/28/java/java封装继承多态/","link":"","permalink":"https://blog.utone.xyz/2015/07/28/java/java%E5%B0%81%E8%A3%85%E7%BB%A7%E6%89%BF%E5%A4%9A%E6%80%81/","excerpt":"","text":"java面向对象三大特性:封装、继承、多态 封装优点主要就是为了体现程序的“高内聚，低耦合”高内聚：类的内部数据细节自己完成，不允许外部干涉；低耦合：仅对外暴露少量的方法用于自己使用 总结就是：把该隐藏的隐藏起来，该暴露的暴露出来 权限修饰符java通过权限修饰符来实现封装性权限从小到大顺序为：private &lt; 缺省 &lt; protected &lt; public4种权限都可以用来修饰类的内部结构：属性、方法、构造器、内部类 继承优点 减少了代码的冗余，提供了代码的复用性 提供了更好的扩展性 为多态性的使用提供前提 1.子类通过继承父类，就获取了父类中声明的结构：属性、方法。即使父类中声明的方法或属性为私有的，我们也认为子类通过继承，获取到了。只是由于封装性的影响，子类不能直接调用这些私的结构而已。2.子类除了可以通过继承获取父类的结构之外，子类还可以定义自己类所特的属性、方法。 多态多态性的使用前提: ①类的继承性 ②方法的重写实际就是:父类的引用指向子类的实例 优点更多是接口的使用。定义一个接口,和抽象方法,可以有多种不同的实现.有了多态,我们在编译时候调动父类声明的方法,但在运行时,实际执行的是子类重写父类的方法，便于合作开发 结束了","categories":[],"tags":[{"name":"java","slug":"java","permalink":"https://blog.utone.xyz/tags/java/"}]},{"title":"linux系统检查","slug":"linux/linux2","date":"2015-07-20T04:00:00.000Z","updated":"2020-06-15T15:25:21.980Z","comments":true,"path":"2015/07/20/linux/linux2/","link":"","permalink":"https://blog.utone.xyz/2015/07/20/linux/linux2/","excerpt":"","text":"系统常用检查命令&emsp;&emsp;磁盘：df -h&emsp;&emsp;内存：free -m&emsp;&emsp;负载：top 1234[root@hadoop001 ~]$ free -m total used free shared buff&#x2F;cache availableMem: 7823 222 6229 257 1371 7096Swap: 0 0 0 &emsp;&emsp;大数据生产服务器swap是设置0。 10也可以（swap作用是将硬盘划分过来作为内存，所以当数据进入swap那么肯定是很缓慢的，所以设置为0） 12345678910111213141516[root@hadoop001 ~]$ df -hFilesystem Size Used Avail Use% Mounted on&#x2F;dev&#x2F;vda1 40G 16G 25G 39% &#x2F;&#x2F;dev&#x2F;vdb1 2T 16G 25G 1% &#x2F;data01&#x2F;dev&#x2F;vdb2 2T 16G 25G 1% &#x2F;data02&#x2F;dev&#x2F;vdb3 2T 16G 25G 1% &#x2F;data03&#x2F;dev&#x2F;vdb4 2T 16G 25G 1% &#x2F;data04不要devtmpfs 3.9G 0 3.9G 0% &#x2F;devtmpfs 3.9G 16K 3.9G 1% &#x2F;dev&#x2F;shmtmpfs 3.9G 258M 3.6G 7% &#x2F;runtmpfs 3.9G 0 3.9G 0% &#x2F;sys&#x2F;fs&#x2F;cgrouptmpfs 783M 0 783M 0% &#x2F;run&#x2F;user&#x2F;1004 （系统启动多少时间，可以看到今天前什么时候重启过）（几个用户在登录）（系统负载，1分钟5分钟15分钟，判断服务器卡不卡） top - 21:20:22 up 7 days, 58 min, 1 user,load average: 0.01, 0.03, 0.05Tasks: 89 total, 1 running, 88 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.2 us, 0.5 sy, 0.0 ni, 99.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.3 stKiB Mem : 8011076 total, 6377388 free, 229060 used, 1404628 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 7265724 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 2374 root 20 0 394348 31376 8608 S 0.3 0.4 41:44.99 jdog-kunlunmirr 1 root 20 0 125356 3796 2508 S 0.0 0.0 1:22.32 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:00.08 ksoftirqd/0 5 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H 6 root 20 0 0 0 0 S 0.0 0.0 0:02.50 kworker/u4:0 系统负载&emsp;&emsp;load average: 0.01, 0.03, 0.05&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; 1min 5min 15min &emsp;&emsp;经验值: 10 生产不用超过这个 ，否则认为服务器就是卡&emsp;&emsp;&emsp;&emsp;a.是你的程序有问题 在大量跑计算&emsp;&emsp;&emsp;&emsp;b.是不是被挖矿 yarn redis 最容易被hacker 攻击&emsp;&emsp;&emsp;&emsp;c.硬件问题 内存条 硬盘 &emsp;&emsp;a，b选项可以看top中%CPU %MEM，哪些进程属于哪个user占用的cpu和内存过高，排查是故意的操作还是程序问题&emsp;&emsp;c选项遇到过硬盘问题导致程序卡主，导致系统负载过高。。正常服务器负载是1,2,3超过5就要观察，超过10就要排查下&emsp;&emsp;c判断硬件问题最简单方法就是重启服务器，如果硬件损坏开机就会有灯闪 软连接 ln软连接就相当于window里的快捷方式,是一个空文件夹 ln -s hadoop-2.6.0-cdh5.16.2 hadoop软连接优点:1.方便版本切换&emsp;&emsp;比如我在shell脚本里写的目录是/home/hadoop/app/hadoop-2.6.0-cdh5.16.2如果我升级hadoop版本,那么我每个shell脚本都需要修改版本非常麻烦若果我用软连接可以直接在shell脚本里写/home/hadoop/app/hadoop 如果hadoop升级了,我只需要先删除hadoop这个软连接,之后重新创建软连接hadoop指向最新的地址就行了不需要挨个修改shell脚本 2.小盘换大盘&emsp;&emsp;比如目录 /app/log/hadoop-hdfs 是在跟目录下面的,企业中根目录一般设置比较小假设为20G 比如hadoop-hdfs这个文件夹使用了18个G,则占用了根目录很多空间此时我想把数据迁移到大盘/data01下,但是还要通过原来的路径访问(因为这样可以不改变程序代码)此时可以用软连接mv /app/log/hadoop-hdfs /data01/ ==&gt;结果:/data01/hadoop-hdfs (数据移动到大盘)ln -s /data01/hadoop-hdfs /app/log/hadoop-hdfs yum安装 yum search httpd yum install httpd centos6:service httpd status|start|stop 1个应用httpd centos7:service httpd status|start|stop 是向下兼容的也能使用systemctl status|start|stop httpd app2 app3 app4 一次性操作多个应用 搜索 卸载:[root@hadoop101 ~]# rpm -qa|grep httphttpd-2.4.6-90.el7.centos.x86_64httpd-tools-2.4.6-90.el7.centos.x86_64[root@hadoop101 ~]# rpm -e 包名称 –nodeps [root@hadoop101 ~]# yum remove httpd-2.4.6-90.el7.centos.x86_64 进程 端口号ps -ef | grep httpkill -9 16629kill -9 16630 16631 16632 16633 16634 根据匹配字段 搜索所有符合的进程 全部杀死但是: 生产慎用 除非你先ps查看 这个关键词搜索的进程 是不是都是你想要杀死的进程保不齐有个其他服务的进程 会造成误杀 生产事故！！！ 12345678[root@hadoop101 ~]# ps -ef|grep httproot 18363 1 0 21:51 ? 00:00:00 &#x2F;usr&#x2F;sbin&#x2F;httpd -DFOREGROUNDapache 18364 18363 0 21:51 ? 00:00:00 &#x2F;usr&#x2F;sbin&#x2F;httpd -DFOREGROUNDapache 18365 18363 0 21:51 ? 00:00:00 &#x2F;usr&#x2F;sbin&#x2F;httpd -DFOREGROUNDapache 18366 18363 0 21:51 ? 00:00:00 &#x2F;usr&#x2F;sbin&#x2F;httpd -DFOREGROUNDapache 18367 18363 0 21:51 ? 00:00:00 &#x2F;usr&#x2F;sbin&#x2F;httpd -DFOREGROUNDapache 18368 18363 0 21:51 ? 00:00:00 &#x2F;usr&#x2F;sbin&#x2F;httpd -DFOREGROUNDroot 18387 15881 0 21:51 pts&#x2F;2 00:00:00 grep --color&#x3D;auto http 上面第二列是进程号 第三列是父进程号 , 可知18363是主进程 想知道进程占用哪个端口号(netstat -nlp| grep 3306（查看端口启动哪个进程）。netstat -nlp| grep mysql（查看mysql占用哪个端口）): 123456789[root@ruozedata001 ~]# netstat -nlp| grep 18670tcp6 0 0 :::80 :::* LISTEN 18670&#x2F;httpd [root@ruozedata001 ~]# [root@ruozedata001 ~]# [root@ruozedata001 ~]# [root@ruozedata001 ~]# netstat -nlp| grep 18671[root@ruozedata001 ~]# netstat -nlp| grep 18672[root@ruozedata001 ~]# netstat -nlp| grep 18673[root@ruozedata001 ~]# 上面可见进程不一定都会起到端口号但是 与其他服务通信 必然需要端口号！！！ 老板: 去打开的应用yyy的网页？你会涉及到哪些Linux命令我知道xxx服务器的ip,之后查询这个yyy对应的端口号:ps -ef|grep yyy ,知道进程号后得到该进程占用的端口号:netstat -nlp|grep pid此时已经得到了端口号之后浏览器里输入http://ip:端口号 (还需要确保能ping 通 ip还有端口号) 细节: 12345678[root@ruozedata001 ~]# netstat -nlp| grep 18670tcp6 0 0 :::80 :::* LISTEN 18670&#x2F;httpd tcp6 0 0 :80 :::* LISTEN 18670&#x2F;httpd tcp6 0 0 192.168.0.3:80 :::* LISTEN 18670&#x2F;httpd ::和0.0.0.0等价于当前机器的iptcp6 0 0 127.0.0.1:80 :::* LISTEN 18670&#x2F;httpd tcp6 0 0 localhost:80 :::* LISTEN 18670&#x2F;httpd 危险: 127.0.0.1和localhost该服务只能自己服务器的里面自己访问自己,外面的服务器访问不了该服务生产中遇到过:服务启动了,防火墙也关了,但是连接不上服务 原因就是是以127.0.0.1的方式启动的,默认配置文件没有修改 测试服务通不通: ping ip测试服务器端口能不能通: telnet ip port yum install -y telnet 学习时：如果都不能通可能防火墙开启。如果是云主机需要开启安全组策略生产中：如果都不能通，直接找linux运维 网络工程师（这个就需要加防火墙策略，防火墙分为硬件或者软件，软件一般便宜一些） 总结:看到错误 Connection refused ping ip (失败未必就是不能连接的有些服务器是禁止ping的(防止黑客扫描) 但是做telnet ip port是可以的) telnet ip port ok 配置企业级别yum源 取代互联网的repo文件的URL (因为我们做的东西都是内网的是物理隔绝的不会连接互联网,) 4.下载wget https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jarcurl https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.12/2.4.5/spark-core_2.12-2.4.5.jar -O spark-core_2.12-2.4.5.jar 5.压缩 解压zip -r xxx.zip xxx/*unzip xxx.zip 解压缩 .zip tar -czvf xxxx.tar.gz xxxx/*tar -zxvf xxxx.tar.gz 解压缩 .tar -C 路径 表示解压到路径下 Examples: tar -cf archive.tar foo bar # Create archive.tar from files foo and bar. tar -tvf archive.tar # List all files in archive.tar verbosely. tar -xf archive.tar # Extract all files from archive.tar. tar -xvf 解压缩 .tar.bz 如何找到命令 which12[root@hadoop101 ~]# which lsalias ls&#x3D;&#39;ls --color&#x3D;auto&#39; 想要命令快速找到 which xxx 来验证，其实就是提前将命令的目录配置在环境变量$PATHecho $PATH 来查看是否将命令的目录配置上！ 定时","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.utone.xyz/tags/linux/"}]},{"title":"linux","slug":"linux/linux1","date":"2015-07-18T04:00:00.000Z","updated":"2020-06-10T05:43:29.599Z","comments":true,"path":"2015/07/18/linux/linux1/","link":"","permalink":"https://blog.utone.xyz/2015/07/18/linux/linux1/","excerpt":"","text":"[root@hadoop102 ~]root : 登录用户时root，root是默认管理员 有最大权限hadoop102 : 指的是机器名~ : 指的是当前该用户的家目录 命令介绍/路径是linux的根路径 1) pwd : 查看当前光标所在的目录 ls命令2) ls : 查看&emsp;&emsp;ls -l 显示额外信息(权限 用户用户组 时间 大小) 等价于:ll&emsp;&emsp;ll -a 也显示隐藏文件夹、文件&emsp;&emsp;&emsp;&emsp;隐藏文件和文件夹都是以‘.’开头，我们可以自己隐藏文件或文件夹&emsp;&emsp;ll -h 查看 文件 的大小&emsp;&emsp;ll -rt : 其中 -r -t是按照时间排序,可以快速找到哪些文件或者文件夹更新了 3) cd : 切换路径&emsp;&emsp;cd .. : 返回上一层目录&emsp;&emsp;cd - : 回退到上一次的目录 家目录root是系统管理员用户 家目录是 /root创建的普通用户xx 家目录是 /home/xx 家目录是 ~ 表示如何进入家目录??&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; ① cd 对应用户家目录&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; ② cd 直接回车&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; ③ cd ~ 4) mkdir : 创建文件夹&emsp;&emsp;需求：创建并联的三个文件夹 mkdir dir1 dir2 dir3&emsp;&emsp;需求：创建串联的三个文件夹 mkdir -p dir4/dir5/dir65) help : 命令帮助&emsp;&emsp;ls –help6) clear : 清空屏幕 或者 Ctrl+l 查看文件/文件夹大小查看文件大小： ll -h 、du -sh查看文件夹大小: du -sh 移动拷贝7) mv : 移动&emsp;&emsp;标准写法 : mv dir1 dir0/dir1&emsp;&emsp;不标准写法: mv dir1 dir08) cp : 拷贝&emsp;&emsp;标准写法 : cp -r dir2 dir0/dir2 //加-r是复制文件夹,如果不是复制文件夹可以不加-r&emsp;&emsp;不标准写法: cp -r dir2 dir0 &emsp;&emsp;mv是始终一份 速度快&emsp;&emsp;cp是两份 速度慢一点 创建文件9) touch : 创建空文件10) vi : 创建文件并编辑&emsp;&emsp; vi 1.log&emsp;&emsp;&emsp;&emsp;默认命令模式 。i建 进入编辑模式 。Esc建 从编辑模式进入命令模式 。Shift+:建从命令行模式进入尾行模式,输入wq退出11) echo : 打印12) &gt; : 创建或者覆盖 (高危命令)&emsp;&emsp;&emsp;&emsp;echo “hello” &gt; 3.log&emsp;&emsp;注意:&gt; 左右两面的空格不能丢13) &gt;&gt; : 追加&emsp;&emsp;&emsp;&emsp;echo “hello world” &gt;&gt; 3.log 查看文件内容14) cat 查看文件内容,文件内容一次全部显示 ctrl+z中断15) more 文件内容一页页往下翻，按空格往下 无法回退 q退出16）less 文件内容 按↑↓键翻行 q退出 用的少&emsp;&emsp;&emsp;&emsp;cat适合文件内容少的。 文件内容稍微多点的用more17）tail&emsp;&emsp;tail -f xxx.log 实时监控文件内容，如果文件移除停止监控,再创建xxx.log文件也不会继续监控&emsp;&emsp;tail -F xxx.log 实时监控文件内容，如果文件移除会继续尝试读取内容不会停止,再创建xxx.log文件后还能继续监控 需求:实时查看文件内容倒数100行&emsp;&emsp;tail -100f xxx.log 注意:此处只能f不能用F 需求:文件内容很多,如何搜索error信息 ‘|’:管道符,前面的结果作为后面的输入‘grep’:过滤&emsp;&emsp;cat xxx.log | grep -A 5 ERROR。error后5行信息&emsp;&emsp;cat xxx.log | grep -B 5 ERROR。error前5行信息&emsp;&emsp;cat xxx.log | grep -C 5 ERROR。error前后各5行信息 但是如果error很多,用cat可能会刷屏,所以优化为: cat xxx.log | grep -C 5 ERROR &gt; 20150718error.log window,linux互传文件安装工具包 yum install -y lrzszsz xxx.log是下载 Linux–》window 下载位置查看CRT的配置rz+回车 是上传 window–》Linux 别名18) 别名 alias 1234567891011[root@ruozedata001 log]# aliasalias cp&#x3D;&#39;cp -i&#39;alias egrep&#x3D;&#39;egrep --color&#x3D;auto&#39;alias fgrep&#x3D;&#39;fgrep --color&#x3D;auto&#39;alias grep&#x3D;&#39;grep --color&#x3D;auto&#39;alias l.&#x3D;&#39;ls -d .* --color&#x3D;auto&#39;alias ll&#x3D;&#39;ls -l --color&#x3D;auto&#39;alias ls&#x3D;&#39;ls --color&#x3D;auto&#39;alias mv&#x3D;&#39;mv -i&#39;alias rm&#x3D;&#39;rm -i&#39;alias which&#x3D;&#39;alias | &#x2F;usr&#x2F;bin&#x2F;which --tty-only --read-alias --show-dot --show-tilde&#39; 上面解释了为什么ls -l可以简写为ll &emsp;&emsp;创建别名 : alias jj=’cd /tmp’ (注意中间不要乱用空格) (只在当前会话生效,想要全局生效需要把命令拷贝到对应用户的配置文件然后生效文件) 全局变量全局变量(配置完了需要生效配置,并且重新打开CRT窗口)全局：/etc/profile 所有用户都可以使用个人: 只能当前用户使用,其他不能使用&emsp;&emsp; ~/.bash_profile&emsp;&emsp; ~/.bashrc (个人环境变量设置推荐这个文件) 场景: ssh 远程执行B机器 命令 找不到 java command not found但是直接登录B机器 命令是能找到的,原因:命令的环境变量配置在.bash_profile 是不正确的。应该配置在.bashrc文件里面 生产中新配置的环境变量应该放到PATH前面,如果配置到后面可能会被前面配置的截胡 配置完后一定要生效文件:&emsp;&emsp;source /etc/profile&emsp;&emsp;source ~/.bash_profile&emsp;&emsp;source ~/.bashrc 检查是否生效：which例子:配置java变量 12345678&#x2F;etc&#x2F;profile添加#envexport JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_45export PATH&#x3D;$JAVA_HOME&#x2F;bin:$PATHsource &#x2F;etc&#x2F;profilewhich java CRT窗口是静态窗口,配置生效后可以重新打开个窗口,拿到最新状态 创建用户19) useradd clouduncle：添加用户clouduncle20) su clouduncle:切换用户clouduncle 历史命令21) history&emsp;自己写的命令不想让别人看到 : history -c (清空history)直连 服务器 history -c可以生效跳板机(vpn) 连接 服务器 history -c可以生效堡垒机 敲一个命令都记录在堡垒机系统里,一般都有可视化web界面,能搜索谁做了什么命令 history -c就不会有效果 高危命令&emsp;&emsp; 千万不要做 rm -rf / 运维可以限制这个命令失效&emsp;&emsp;删除文件: rm 11.log 会询问是否删除&emsp;&emsp;不询问直接删除文件: rm -f 11.log&emsp;&emsp;删除文件夹: rm -r dir1&emsp;&emsp;不询问直接删除文件夹: rm -rf dir1 场景: 注意&emsp;&emsp;脚本里&emsp;&emsp;LOG_PATH=/xxx/yyy&emsp;&emsp;&emsp;&emsp;业务逻辑判断去赋值,如果漏掉了一种没有赋值那么LOG_PATH为空&emsp;&emsp;rm -rf ${LOG_PATH} /* ==&gt; rm -rf /* 怎么避免?每次执行这个命令之前 , 先判断${LOG_PATH}是否存在然后才能执行 发生了怎么办？如果不小心执行了这个操作且没有备份，则马上停止所有服务不要往硬盘上再写数据了，让运维去拿着硬盘去维修公司恢复数据。最好是运维做出这种命令限制 用户和用户组（一般都是由运维操作，除非平台运维都由自己团队接过来）用户1）创建普通用户clouduncle: useradd clouduncle&emsp;&emsp;同时也会创建clouduncle的用户组,设置clouduncle用户的组为clouduncle,且把clouduncle用户组设置为主组同时也创建家目录 /home/clouduncle &emsp;&emsp;用户存储信息一般记录在: /etc/passwd&emsp;&emsp;用户组存储信息一般记录在: /etc/group 2）查看用户clouduncle信息：id clouduncle 3) 删除用户clouduncle：userdel clouduncle删除用户后, /home/clouduncle文件夹还存在,之后手动删除该文件夹下的所有文件包括隐藏文件模拟全局变量丢失。重新创建clouduncle用户出现问题: 123456789101112[root@hadoop102 home]# su cloudunclebash-4.1$ [root@hadoop102 clouduncle]# ll -a &#x2F;etc&#x2F;skel&#x2F;.*总用量 36drwxr-xr-x. 4 root root 4096 2月 8 2018 .drwxr-xr-x. 107 root root 12288 5月 25 21:03 ..-rw-r--r--. 1 root root 18 5月 11 2016 .bash_logout-rw-r--r--. 1 root root 176 5月 11 2016 .bash_profile-rw-r--r--. 1 root root 124 5月 11 2016 .bashrcdrwxr-xr-x. 2 root root 4096 11月 12 2010 .gnome2drwxr-xr-x. 4 root root 4096 2月 8 2018 .mozilla 出现上述问题原因是当前用户的家目录里面缺少了全局变量信息,此时如下操作: 1[root@hadoop102 clouduncle]# cp &#x2F;etc&#x2F;skel&#x2F;.* .&#x2F; 4)切换用户: su&emsp;&emsp;推荐: su - clouduncle (其中’-‘代表该用户切换到家目录,且执行环境变量文件) 5)普通用户临时使用root的最大权限 : sudo&emsp;&emsp;root用户下&emsp;&emsp;&emsp;&emsp;vi /etc/sudoers&emsp;&emsp;&emsp;&emsp;添加 : clouduncle ALL=(root) NOPASSWD:ALL 生产中可以要求运维给自己linux用户加一个sudo的权限 设置密码1)修改用户密码: passwd&emsp;&emsp;passwd+回车 修改当前光标所属用户密码&emsp;&emsp;passwd clouduncle 修改clouduncle的密码 2)./etc/passwd文件下面两种情况切换用户都会失败&emsp;&emsp;clouduncle:x:1004:1005::/home/clouduncle:/sbin/nologin 提示&emsp;&emsp;clouduncle:x:1004:1005::/home/clouduncle:/usr/bin/false 没提示 CDH平台很多用户如 hdfs yarn hivesu - yarn是不成功的,原因是yarn的/sbin/nologin /usr/bin/false 需要改成 /bin/bash 用户组1)创建组bigdata : groupadd bigdata2)查看组: cat /etc/group|grep bigdata3)将用户分配到bigdata组: (usermod –help查询方法) 123[root@hadoop102 clouduncle]# usermod -a -G bigdata clouduncle[root@hadoop102 clouduncle]# id clouduncle uid&#x3D;501(clouduncle) gid&#x3D;501(clouduncle) 组&#x3D;501(clouduncle),502(bigdata) 4)设置bigdata为用户clouduncle主组(gid): usermod -g bigdata clouduncle 123456[root@hadoop102 clouduncle]# usermod -g bigdata clouduncle[root@hadoop102 clouduncle]# id clouduncleuid&#x3D;501(clouduncle) gid&#x3D;502(bigdata) 组&#x3D;502(bigdata)[root@hadoop102 clouduncle]# usermod -a -G clouduncle clouduncle[root@hadoop102 clouduncle]# id clouduncleuid&#x3D;501(clouduncle) gid&#x3D;502(bigdata) 组&#x3D;502(bigdata),501(clouduncle) 权限-rw-r–r– 1 root root 9 Apr 18 20:50 22.logdrwxr-xr-x 2 root root 6 Apr 15 22:12 dir3 &emsp;&emsp;第一个字母：d文件夹 -文件 l连接 &emsp;&emsp;后面9个字母,3个字母为一组：r：read 读权限 代表数字是4w：write 写权限 代表数字是2x： 执行 代表数字是1-： 没权限 代表数字是0 占位 7=4 2 15=4 16=4 2 &emsp;&emsp;rw- 第一组 6 代表文件或文件夹的所属用户，读写权限&emsp;&emsp;r– 第二组 4 代表文件或文件夹的所属用户组，读权限&emsp;&emsp;r– 第三组 4 代表其他用户组的用户对这个文件或文件夹，读权限 例子: rw-r- -r- - root root 22.log&emsp;&emsp;可以看到22.log 属于root用户 属于root用户组&emsp;&emsp;root用户看前三个对22.log有读和写的权限。root用户组看中间对22.log有读权限。其他用户组的用户看后三个，对22.log有读权限 关于权限：&emsp;&emsp;chmod -R 777 文件或文件夹 (777分别对应所属用户、所属用户组、其他用户组的用户 所拥有的权限)(7=4+2+1)&emsp;&emsp;chown -R 用户:用户组 文件或文件夹其中-R ： 以递归方式更改所有的文件及子目录chmod是修改文件或文件夹对不同用户不同用户组的权限chown是给文件夹或文件修改所属用户和用户组 案例: 12345678910[root@hadoop102 tmp]# vi clouduncle.logwww.baidu.com[clouduncle@hadoop102 tmp]$ cat clouduncle.log www.baidu.com收回其他组的r权限 [root@hadoop102 tmp]# chmod 640 clouduncle.log[clouduncle@hadoop102 tmp]$ cat clouduncle.log cat: clouduncle.log: Permission denied 搜索场景：接手大数据平台，服务器登录，大数据组件安装目录在哪？&emsp;&emsp; find / -name ‘*hadoop*‘ (/表示从根目录开始,*是通配符) 如果生产服务器挂10个盘,那么每个盘都要扫描&emsp;&emsp; find /opt/module -name ‘*hadoop’ 补充:history 命令ps -ef 查看进程 vi&emsp;&emsp;默认命令模式 。i建 进入编辑模式 。Esc建 从编辑模式进入命令模式 。Shift+:建从命令行模式进入尾行模式,输入wq退出，wq!强制保存退出，q!强制退出不保存正常编辑一个文件，要正常退出 wq&emsp;&emsp;如果编辑的时候进程被杀死没正常退出:&emsp;&emsp;-rw-r–r– 1 root root 16 Apr 19 21:26 2.log&emsp;&emsp;-rw-r–r– 1 root root 12288 Apr 19 21:31 .2.log.swp&emsp;&emsp;之后再次编辑会无法编辑出现found a swap file by the name “.2.log.swp”&emsp;&emsp;解决方法:删除”.2.log.swp”文件,这是个隐藏文件需要ll -a才能看到 &emsp;&emsp;注意粘贴的坑,如果在命令模式的时候直接粘贴会出现第一行内容丢失 不完整,所以必须先进入编辑模式才能粘贴 如果想要搜索文件XXX内容可以 尾行模式输入 /XXX如果想要显示行号,尾行模式输入 set nu 设置行号 , set nonu 取消行号如果想要知道光标所在的行,尾行模式输入 f 命令模式常用快捷方式dd 删除光标当前行dG 删除光标当前及以下的所有行ndd 删除光标当前行及以下n行 gg 光标跳转到第一行第一个字母G 光标跳转到最后一行第一个字母shift + $ 行尾 场景：清空这个文件内容，从另外一个文件内容 拷贝过来gg–》dG –》 i –&gt;鼠标右键单击 粘贴上 清空补充：cat /dev/null &gt; 1.logecho “” &gt; 2.logtrue &gt; 1.log 也是清空文件内容 0字节 [root@ruozedata001 ~]# lltotal 16-rw-r–r– 1 root root 0 Apr 19 21:58 1.log-rw-r–r– 1 root root 1 Apr 19 21:58 2.log 可以看到第一种方式清空是0字节，第二种方式清空还有一个字节其实并没有真正清空还有个空白行 场景:shell脚本，对数据文件清空操作，根据字节大小判断是否清空完成echo “” &gt; 2.logif filezie &gt; 0 then 业务不操作 else 2.log 灌业务数据 上面清空操作是错误文件自己数不会为0，永远不会进入业务操作 linux两个机制&emsp;&emsp; oom-killer机制：当linux服务器某个进程使用内存超标 Linux机器为了保护自己，会主动杀死进程，释放内存。这个就是本来我的进程好好的，过了一夜莫名其妙的挂了，可能就是触发了这个机制。&emsp;&emsp; /tmp目录 30天自动删除机制 结束了","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://blog.utone.xyz/tags/linux/"}]}],"categories":[],"tags":[{"name":"flink","slug":"flink","permalink":"https://blog.utone.xyz/tags/flink/"},{"name":"故障","slug":"故障","permalink":"https://blog.utone.xyz/tags/%E6%95%85%E9%9A%9C/"},{"name":"hdfs","slug":"hdfs","permalink":"https://blog.utone.xyz/tags/hdfs/"},{"name":"sqoop","slug":"sqoop","permalink":"https://blog.utone.xyz/tags/sqoop/"},{"name":"git","slug":"git","permalink":"https://blog.utone.xyz/tags/git/"},{"name":"Hive","slug":"Hive","permalink":"https://blog.utone.xyz/tags/Hive/"},{"name":"hadoop","slug":"hadoop","permalink":"https://blog.utone.xyz/tags/hadoop/"},{"name":"Yarn","slug":"Yarn","permalink":"https://blog.utone.xyz/tags/Yarn/"},{"name":"MapReduce","slug":"MapReduce","permalink":"https://blog.utone.xyz/tags/MapReduce/"},{"name":"java","slug":"java","permalink":"https://blog.utone.xyz/tags/java/"},{"name":"linux","slug":"linux","permalink":"https://blog.utone.xyz/tags/linux/"}]}