
<!DOCTYPE html>
<html lang="zh-CN">


<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">
  <meta name="theme-color" content="#202020"/>
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
  
  
    <meta name="keywords" content="hadoop,hdfs," />
  

  
    <meta name="description" content="一起成长" />
  
  
  <link rel="icon" type="image/x-icon" href="/logo.png">
  <title>HDFS [ 小彩鸟 ]</title>
  
    <!-- stylesheets list from config.yml -->
    
      <link rel="stylesheet" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css">
    
      <link rel="stylesheet" href="/css/xoxo.css">
    
  
<meta name="generator" content="Hexo 4.2.1"></head>


<body>
  <div class="nav-container">
    <nav class="home-menu pure-menu pure-menu-horizontal">
  <a class="pure-menu-heading" href="/">
    <img class="avatar" src="/images/logo.png">
    <span class="title">小彩鸟</span>
  </a>

  <ul class="pure-menu-list clearfix">
      
          
            <li class="pure-menu-item"><a href="/" class="pure-menu-link">首页</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/archives" class="pure-menu-link">归档</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/tags" class="pure-menu-link">标签</a></li>
          
      
          
            <li class="pure-menu-item"><a href="/search" class="pure-menu-link">搜索</a></li>
          
      
  </ul>
   
</nav>
  </div>

  <div class="container" id="content-outer">
    <div class="inner" id="content-inner">
      <div class="post-container">
  <article class="post" id="post">
    <header class="post-header text-center">
      <h1 class="title">
        HDFS
      </h1>
      <span>
        
        <time class="time" datetime="2016-06-01T04:00:00.000Z">
        2016-06-01
      </time>
        
      </span>
      <span class="slash">/</span>
      <span class="post-meta">
      <span class="post-tags">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hdfs/" rel="tag">hdfs</a></li></ul>
      </span>
    </span>
      <span class="slash">/</span>
      <span class="read">
      <span id="busuanzi_value_page_pv"></span> 点击
    </span>
      <span class="slash">/</span>
    </header>

    <div class="post-content">
      <p>大数据:存储（hdfs），计算（MR，hive，spark，flink），资源和作业调度（yarn）。其中存储是最重要的，一切都是建立在存储的数据上的。</p>
<h1 id="HDFS组成"><a href="#HDFS组成" class="headerlink" title="HDFS组成"></a>HDFS组成</h1><p>&emsp;&emsp;有三个java进程，分别是NameNode（名称节点），DataNode（数据节点），SecondaryNamenode（第二名称节点，生产中不用）<br>HDFS是主从架构（老大不干活只是负责管理，干活的都是小弟）</p>
<h2 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h2><p>&emsp;&emsp;NameNode主要维护 a.文件的名称  b.文件的路径  c.文件的属性，权限，创建时间，副本数  d.一个文件包括其副本被对应切割哪些数据块，这些数据块对应被分布在哪些节点上，blockmap块映射:当然NN节点是不会持久化存储blockmap这种映射关系，是通过集群的启动和运行时DN定期发送blockreport给nn，然后nn就在内存中动态维护这种映射关系(可能实时对数据写入，如果元数据信息持久化维护到磁盘是不利于动态更新的)</p>
<p>&emsp;&emsp;NameNode被格式化后，将在参数【core-site.xml文件里定义hadoop.tmp.dir】/dfs/name/current/目录中生成如下文件：fsimage，editlog，seen_txid</p>
<p>下图是NN的编辑日志和镜像文件：</p>
<p><img src="/img/nn_EditsAndFsimage.png" alt="NN路径下的编辑日志和镜像文件"></p>
<h3 id="镜像文件（fsimage）"><a href="#镜像文件（fsimage）" class="headerlink" title="镜像文件（fsimage）"></a>镜像文件（fsimage）</h3><ul>
<li>HDFS文件系统元数据的一个永久性检查点，其中包含HDFS文件系统的所有目录和文件idnode的序列化信息。Fsimage只会保留最新的和第二新的，旧的全部删除</li>
</ul>
<h3 id="编辑日志（edits）"><a href="#编辑日志（edits）" class="headerlink" title="编辑日志（edits）"></a>编辑日志（edits）</h3><ul>
<li>存放HDFS文件系统的所有更新操作的路径，一个小时一次滚动，每次都会生成一个已经写完的文件，文件系统客户端执行的所有写操作首先会被记录到edits文件中。edits合并后也不会删除</li>
</ul>
<h3 id="seen-txid"><a href="#seen-txid" class="headerlink" title="seen_txid"></a>seen_txid</h3><ul>
<li>该文件保存的是一个数字，就是最后一个edits_的数字</li>
</ul>
<h3 id="nn工作流程"><a href="#nn工作流程" class="headerlink" title="nn工作流程"></a>nn工作流程</h3><p>&emsp;&emsp;元数据都放在内存中，但是怎么持久化呢，不能一关机元数据就都没了</p>
<p>&emsp;&emsp;元数据在内存中很大，持久化我们想到了Redis面临大块内存持久化策略：RDB和AOF。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Redis          Hadoop</span><br><span class="line">RDB            Fsimage</span><br><span class="line">AOF            edits.log</span><br></pre></td></tr></table></figure>

<ul>
<li>RDB：持久化过程时间更长，不能每次操作都生成一次RDB需要隔一段时间或隔一定操作量生成一次RDB。好处是加载快–&gt;加载快,生成慢,安全性略低</li>
<li>AOD：记录了每条操作，占用的空间会更大。好处是每步操作都会记录，更安全。–&gt;加载慢,生成快,安全性高</li>
</ul>
<p>&emsp;&emsp;Hadoop也是有两种类型解决方案，HDFS对数据一致性和安全性非常高，如果持久化采用类AOF策略：edits.log这个文件实时记录了hadoop所有的元数据变动，确保数据安全性，缺点就是加载慢，如果我重启HDFS，会读取非常慢。如果持久化采用类RDB策略：Fsimage这个生成很慢，在写入过程中由于是做整个内存镜像所以只能对外提供读服务，不能提供写服务，因为正在持久化内存数据那么这个数据不能动。企业中Hadoop读写非常频繁，不能出现空档期</p>
<p>&emsp;&emsp;所以hadoop自己持久化edits.log但是还希望有Fsimage。所以2nn出来了，2nn就是帮助nn把edits.log合并成Fsimage</p>
<p><img src="/img/nn%E6%A8%A1%E5%BC%8F.png" alt="nn模式"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1）第一阶段：namenode启动</span><br><span class="line">（1）第一次启动namenode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</span><br><span class="line">（2）客户端对元数据进行增删改的请求</span><br><span class="line">（3）namenode记录操作日志，更新滚动日志。</span><br><span class="line">（4）namenode在内存中对数据进行增删改查</span><br><span class="line">2）第二阶段：Secondary NameNode工作</span><br><span class="line">	（1）Secondary NameNode询问namenode是否需要checkpoint。直接带回namenode是否检查结果。</span><br><span class="line">	（2）Secondary NameNode请求执行checkpoint。</span><br><span class="line">	（3）namenode滚动正在写的edits日志</span><br><span class="line">	（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode</span><br><span class="line">	（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</span><br><span class="line">	（6）生成新的镜像文件fsimage.chkpoint</span><br><span class="line">	（7）拷贝fsimage.chkpoint到namenode</span><br><span class="line">	（8）namenode将fsimage.chkpoint重新命名成fsimage</span><br></pre></td></tr></table></figure>

<h2 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h2><p>&emsp;&emsp;存储数据块和数据块的校验和(校验和数据块第一次进入hdfs时候，会给这个块算一个md5的值，后续集群会定期通过初始存储的md5值和现在存储这个块的md5值比较来查看块是否损坏，损坏后集群可以修复)<br>&emsp;&emsp;作用:<br>&emsp;&emsp;a.每隔3秒发送心跳给NN，告诉NN自己还活着  调整参数:dfs.heartbeat.interval 默认是3s<br>&emsp;&emsp;b.dfs.blockreport.intervalMsec(datanode向namenode报告块信息的时间间隔即blockreport)  默认是21600000ms=6小时  ， dfs.datanode.directoryscan.interval(datanode进行内存和磁盘数据集块校验，更新内存中的信息和磁盘中信息的不一致情况)  21600s=6小时  (总结，在六小时内把差异化目录扫描，然后把块报告发送出去，这两个缺一不可..未来调优时候这两个参数值同时调调成一样..这两个参数控制隔多久hdfs检测并且自我修复)</p>
<p>&emsp;&emsp;生产中偶尔监控到块损坏，可以不用自己修复，集群会自己修复 (生产案例，丢失了将近10000块，恢复时间一天多)<br>&emsp;&emsp;上传文件切的块在linux上存储的位置是:/home/hadoop/tmp/dfs/data1(有多个data1，data2，…)/current/BP-1240559480-192.168.1.102-1529720722096/current/finalized/subdir0/subdir0/(有多个subdir0，subdir1，…)</p>
<h3 id="块-block"><a href="#块-block" class="headerlink" title="块 block"></a>块 block</h3><p><em>块是物理上的概念，切片是逻辑上的概念，不要混为一谈</em></p>
<h4 id="块和切片的区别"><a href="#块和切片的区别" class="headerlink" title="块和切片的区别"></a>块和切片的区别</h4><p>块：指的是文件在HDFS上的存储，将要存储的数据分成一块又一块，然后放在HDFS上存储；</p>
<p>切片：指的是MapReduce阶段的FileInputFormat阶段对于数据块的切片，默认的切片大小是块大小。然后每一个切片分配一个map(mapTask)处理</p>
<h4 id="块大小"><a href="#块大小" class="headerlink" title="块大小"></a>块大小</h4><p>&emsp;&emsp;HDFS中的文件在物理上是分块存储的。HDFS存储大文件是利好，存储小文件是损害自己的。即适合存储大文件，不适合存储小文件（不代表不能存储小文件），因为NN是存储元数据信息的，一亿个10KB的文件在hdfs上存储会生成三亿个block(默认副本数是3)，如果将这些文件压缩成1KW个100M的文件在hdfs上存储会生成3KW个block，3KW肯定比3亿对NN压力小，所以生产中尽量不要有小文件存在<br>&emsp;&emsp;HDFS上的文件会被切割成块，块的大小在<em>hdfs-site.xml</em>文件里配置，配置参数是<em>dfs.blocksize 默认参数:134217728 =128M</em>（当然也可以调整为256M，这个可以进一步降低NN的压力）<br>案例: 260m文件切割为 128m，128m，4m这三个块</p>
<h4 id="副本数"><a href="#副本数" class="headerlink" title="副本数"></a>副本数</h4><p>&emsp;&emsp;副本就是为了保证数据存储的可靠性防止单点故障造成数据丢失，给每个块都做备份分布式存储到不同的机器上。如副本数设为3，那么这个块一共存储3份在hdfs上<br>&emsp;&emsp;配置参数是 <em>dfs.replication，生产上HDFS是集群的，这个参数一般设置为3</em></p>
<p>最开始hadoop默认块大小是64M，后来调整为128M，为什么？<br>比如mv文件是260M</p>
<ul>
<li>如果块大小是64M。260/64=4余4M，相当于划分为五个块分别为:b1(64M)，b2(64M)，b3(64M)，b4(64M)，b5(4M)</li>
<li>如果块大小是128M。260/128=2余4M，相当于划分为三个块分别为:b1(128M)，b2(128M)，b3(4M)</li>
</ul>
<p>总结：无论块大小是64M还是128M，存储实际大小都是:260M*3=780M  (副本数都是3) 。区别就是分成不同的块，5个块的元数据信息维护要比3个块的元数据信息维护重，会对NN造成压力</p>
<h3 id="hdfs写流程"><a href="#hdfs写流程" class="headerlink" title="hdfs写流程"></a>hdfs写流程</h3><p>网上：<a href="https://www.imbajin.com/2020-01-18-HDFS%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BClient%E5%86%99%E6%B5%81%E7%A8%8B%E4%B8%80/" target="_blank" rel="noopener">https://www.imbajin.com/2020-01-18-HDFS%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8BClient%E5%86%99%E6%B5%81%E7%A8%8B%E4%B8%80/</a><br><img src="/img/HDFS%E5%86%99%E6%B5%81%E7%A8%8B.png" alt="NN路径下的编辑日志和镜像文件"><br>&emsp;&emsp;1.源码中HDFS Client调用DistributedFileSystem.create(filePath)方法，去和NN进行通信，申请上传文件！NN 会去check这个路径的文件名是否已经存在，是否有权限能够创建这个文件！如果都满足，就创建空的文件。create方法返回值是该文件输出流FSDataOutputStream<br>&emsp;&emsp;2.之后client向nn请求上传第一个block，nn返回对应的dn比如三个副本分别是dn1，dn2，dn3。之后dn1，dn2，dn3会依次建立通道，client收到应答后，说明通道建立成功<br>&emsp;&emsp;3.Client调用FSData<em>OutputStream</em>对象的write方法，传输packet（64K一个），当dn1收到后一边本地落盘一边发给dn2，dn2也同理，dn3落盘成功后就返回一个ack packet确定包给DN2节点，当DN2节点收到这个ack packet确定包加上自己也是写完了，就返回一个ack packet确定包给第一个DN1节点，当DN1节点收到这个ack packet确定包加上自己也是写完了，将ack packet确定包返回给【FSDataOutputStream】对象，每当发送一个packet就将该packet放到ack队列中，若收到ack就说明成功了，则删除ack队列中的对应packet。当前block传输完成后, client会接着再申请一个新的block, 直到所有数据传输完成<br><img src="/img/%E5%86%99%E5%85%A5%E6%A8%A1%E5%9D%97.png" alt="写入模块"><br>&emsp;&emsp;4.当所有的块全部写完， client调用 FSDataOutputStream对象的close方法，关闭输出流。再次调用FileSystem.complete方法，告诉nn文件写成功！</p>
<p>注:<br>伪分布式 1台dn     副本数参数必然是设置1，dn挂了，肯定不能写入</p>
<p>生产分布式 3台dn   副本数参数设置3，dn挂了，肯定不能写入</p>
<p>生产分布式 &gt;3台dn  副本数参数设置3，dn挂了，能写入</p>
<p>总结就是:存活的alive dn节点数&gt;=副本数 就能写成功</p>
<h3 id="hdfs读流程"><a href="#hdfs读流程" class="headerlink" title="hdfs读流程"></a>hdfs读流程</h3><p><img src="/img/HDFS%E8%AF%BB%E6%B5%81%E7%A8%8B.png" alt="NN路径下的编辑日志和镜像文件"><br>&emsp;&emsp;1.Client调用FileSystem的open(filePath)，与NN进行RPC通信，namenode检查文件是否存在，如果存在则client会请求下载第一个block，nn返回对应的dn列表<br>&emsp;&emsp;2.Client调用FSData<em>InputStream</em>对象的read方法去与第一个块的最近的DN进行读取，读取完成后，会继续请求读取第二个块以此类推。<br>&emsp;&emsp;3.Client调用FSDataInputStream对象close方法，关闭输入流。</p>
<h3 id="hdfs副本放置策略-（面试和生产都需要）"><a href="#hdfs副本放置策略-（面试和生产都需要）" class="headerlink" title="hdfs副本放置策略 （面试和生产都需要）"></a>hdfs副本放置策略 （面试和生产都需要）</h3><p>生产上读写操作 尽量选择 某个DN节点，因为这样第一个副本可以减少网络消耗<br>第一个副本：<br>放置在上传的DN节点；<br>假如是非DN节点，就随机挑选一个磁盘不太慢，cpu不太忙的节点；</p>
<p>第二个副本：<br>放置在第一个副本不同的机架上的某个DN节点。一般一个机架最多装10台机器，一般7，8台。生产上也没有配置机架的配置，不配置就认为所有的机器在一个整体的机架。</p>
<p>第三个副本:<br>与第二个副本相同机架的不同节点上。</p>
<h2 id="SecondaryNamenode"><a href="#SecondaryNamenode" class="headerlink" title="SecondaryNamenode"></a>SecondaryNamenode</h2><p>HA中StandBy nn会干2NN干的活,而且还会干的更好是实时备份<br>&emsp;&emsp;SNN主要是为了解决NN的单点故障，做1小时的备份，在NN发生故障的时候可以从SNN恢复数据，虽然能够减轻单点故障带来的丢数风险，但是在生产上还是不允许使用SNN<br>&emsp;&emsp;方法就是将nn的fsimage和editlog定期拿过来合并，备份，推送</p>
<p>checkPoint参数，两个参数是或的关系：</p>
<ul>
<li>dfs.namenode.checkpoint.period  3600s     （做检查点（checkpoint）间隔时间）</li>
<li>dfs.namenode.checkpoint.txns    1000000   （写数据条数，如果写到了1000000也会做checkpoint）</li>
</ul>
<p>&emsp;&emsp;如果11:00 SNN备份了，但是11:30的时候突然NN节点磁盘故障，此时拿SNN的最新的fsimage文件恢复，那么只能恢复11点之前的数据。虽然11:00-11:30的数据还在HDFS上，但是这些数据的元数据信息丢失了，那么无法使用了。</p>
<p>&emsp;&emsp;因为还是存在丢失部分元数据信息的可能，所以生产上不会使用SNN，是启动另一个NN进程，这个进程实时备份，实时准备替换NN，变为活动的NN。。这个架构叫做<em>HDFS HA</em></p>
<h1 id="HDFS命令"><a href="#HDFS命令" class="headerlink" title="HDFS命令"></a>HDFS命令</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 hadoop]$ ll</span><br><span class="line">total 132</span><br><span class="line">drwxr-xr-x  2 hadoop hadoop   128 Jun  3  2019 bin   可执行脚本  命令</span><br><span class="line">drwxr-xr-x  2 hadoop hadoop  4096 Jun  3  2019 bin-mapreduce1</span><br><span class="line">drwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2019 cloudera</span><br><span class="line">drwxr-xr-x  6 hadoop hadoop   105 Jun  3  2019 etc    配置文件夹，也有的叫conf&#x2F;config</span><br><span class="line">drwxr-xr-x  5 hadoop hadoop    40 Jun  3  2019 examples  案例</span><br><span class="line">drwxr-xr-x  3 hadoop hadoop    27 Jun  3  2019 examples-mapreduce1</span><br><span class="line">drwxr-xr-x  2 hadoop hadoop   101 Jun  3  2019 include</span><br><span class="line">drwxr-xr-x  3 hadoop hadoop    19 Jun  3  2019 lib       jar包</span><br><span class="line">drwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2019 libexec</span><br><span class="line">-rw-r--r--  1 hadoop hadoop 85063 Jun  3  2019 LICENSE.txt</span><br><span class="line">drwxrwxr-x  3 hadoop hadoop  4096 May  9 22:39 logs      日志的</span><br><span class="line">-rw-r--r--  1 hadoop hadoop 14978 Jun  3  2019 NOTICE.txt</span><br><span class="line">drwxrwxr-x  2 hadoop hadoop    40 May  6 23:13 output</span><br><span class="line">-rw-r--r--  1 hadoop hadoop    41 May  9 21:43 part-r-00000</span><br><span class="line">-rw-r--r--  1 hadoop hadoop  1366 Jun  3  2019 README.txt</span><br><span class="line">drwxr-xr-x  3 hadoop hadoop  4096 Jun  3  2019 sbin    启动 停止 重启脚本</span><br><span class="line">drwxr-xr-x  4 hadoop hadoop    29 Jun  3  2019 share</span><br><span class="line">drwxr-xr-x 18 hadoop hadoop  4096 Jun  3  2019 src</span><br></pre></td></tr></table></figure>
<p><em>早期的命令都是hadoop fs现在都是hdfs dfs 。看hadoop脚本都是调用的的同一个shell，所以用哪个都一样</em>，脚本如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls &#x2F;</span><br><span class="line"> if [ &quot;$COMMAND&quot; &#x3D; &quot;fs&quot; ] ; then</span><br><span class="line">      CLASS&#x3D;org.apache.hadoop.fs.FsShell</span><br><span class="line"></span><br><span class="line">hdfs dfs -ls &#x2F;</span><br><span class="line"> elif [ &quot;$COMMAND&quot; &#x3D; &quot;dfs&quot; ] ; then</span><br><span class="line">      CLASS&#x3D;org.apache.hadoop.fs.FsShell</span><br></pre></td></tr></table></figure>
<h2 id="hadoop常用命令"><a href="#hadoop常用命令" class="headerlink" title="hadoop常用命令:"></a>hadoop常用命令:</h2><p>1.显示目录信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop102 hadoop]$ hadoop fs -ls &#x2F;</span><br><span class="line">[hadoop@hadoop102 hadoop]$ hadoop fs -ls -R &#x2F;</span><br><span class="line"></span><br><span class="line">hadoop fs -ls  </span><br><span class="line">	其实访问是这个路径  &#x2F;user&#x2F;hadoop(linux当前用户)&#x2F;</span><br><span class="line">hadoop fs -ls  &#x2F;</span><br><span class="line">     其实直接 &#x2F; 是因为默认加上了core-site.xml参数里的fs.default参数对应的值hdfs:&#x2F;&#x2F;hadoop102:9000</span><br><span class="line">hadoop fs -ls hdfs:&#x2F;&#x2F;hadoop102:9000&#x2F;</span><br></pre></td></tr></table></figure>

<p>2.在hdfs上创建目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop102 hadoop]$ hadoop fs -mkdir &#x2F;shuihu</span><br><span class="line">	创建多级文件夹</span><br><span class="line">	[hadoop@hadoop102 hadoop]$ hadoop fs -mkdir -p &#x2F;shuihu&#x2F;liangshan&#x2F;sj</span><br></pre></td></tr></table></figure>

<p>3.从本地剪切到hdfs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop102 hadoop]$ hadoop fs -moveFromLocal dc.txt &#x2F;shuihu&#x2F;liangshan&#x2F;sj</span><br></pre></td></tr></table></figure>

<p>4.从本地文件系统中拷贝文件到hdfs路径去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop102 hadoop]$ hadoop fs -put NOTICE.txt &#x2F;</span><br></pre></td></tr></table></figure>

<p>5.从hdfs拷贝到本地(此时的本地是linux)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop102 hadoop]$ hadoop fs -get  &#x2F;shuihu&#x2F;liangshan&#x2F;sj&#x2F;dc.txt .&#x2F;</span><br></pre></td></tr></table></figure>

<p>6.从hdfs的一个路径拷贝到hdfs的另一个路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop102 hadoop]$ hadoop fs -cp &#x2F;shuihu&#x2F;liangshan&#x2F;sj&#x2F;dc.txt &#x2F;shuihu&#x2F;liangshan&#x2F;</span><br></pre></td></tr></table></figure>

<p>7.在hdfs上移动文件(相当于剪切)  (尽量少用，可以先复制然后校验复制的是否完整后，在回来删除数据，防止剪切时候出问题数据丢失)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop102 hadoop]$ hadoop fs -mv &#x2F;shuihu&#x2F;liangshan&#x2F;dc.txt &#x2F;</span><br></pre></td></tr></table></figure>

<p>8.统计hdfs可用空间大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop102 hadoop]$ hadoop fs -df -h &#x2F;</span><br></pre></td></tr></table></figure>

<p>9.删除文件或文件夹</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br><span class="line">[hadoop@hadoop102 hadoop]$ hadoop fs -rm &#x2F;dc.txt </span><br><span class="line">[hadoop@hadoop102 hadoop]$ hadoop fs -rm -r &#x2F;my</span><br><span class="line"></span><br><span class="line">生产上:  1.开启回收站！+回收站的有效期至少7天(具体几天自己配置，建议至少7天)！！ (配置文件是core-site.xml中)</span><br><span class="line">         2.慎用-skipTrash，否则数据直接删除不会进入回收站！！</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs -rm &#x2F;1.log</span><br><span class="line">20&#x2F;05&#x2F;12 21:45:27 INFO fs.TrashPolicyDefault: Moved: &#39;hdfs:&#x2F;&#x2F;hadoop001:9000&#x2F;1.log&#39; to trash at: hdfs:&#x2F;&#x2F;hadoop001:9000&#x2F;user&#x2F;hadoop&#x2F;.Trash&#x2F;Current&#x2F;1.log</span><br><span class="line"></span><br><span class="line">注:</span><br><span class="line">hdfs:&#x2F;&#x2F;hadoop001:9000&#x2F;user&#x2F;hadoop&#x2F;.Trash&#x2F;Current&#x2F;1.log 这个文件hdfs将会保留7天(参数配置)，7天后会被删除</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@hadoop001 hadoop]$ hdfs dfs -rm -skipTrash  &#x2F;2.log</span><br><span class="line">Deleted &#x2F;2.log</span><br></pre></td></tr></table></figure>

<p>10.-chown -chmod跟linux一样</p>
<p>&emsp;&emsp;-get 等同于-copyToLocal     (下载)<br>&emsp;&emsp;-put 等同于-copyFromLocal   (上传)</p>
<p>11.检查hadoop支持的压缩:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@ruozedata001 hadoop]$ hadoop checknative</span><br><span class="line">20&#x2F;05&#x2F;12 21:53:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Native library checking:</span><br><span class="line">hadoop:  false </span><br><span class="line">zlib:    false </span><br><span class="line">snappy:  false </span><br><span class="line">lz4:     false </span><br><span class="line">bzip2:   false </span><br><span class="line">openssl: false </span><br><span class="line">20&#x2F;05&#x2F;12 21:53:26 INFO util.ExitUtil: Exiting with status 1</span><br></pre></td></tr></table></figure>
<p>12.得到hdfs某一路径下大小:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">该路径下各个文件夹分别大小:</span><br><span class="line">hadoop fs -du -h &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;ori_partitioned</span><br><span class="line">该路径下所有文件夹总共大小:</span><br><span class="line">hadoop fs -du -s &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;ori_partitioned</span><br></pre></td></tr></table></figure>

<p>13.得到hdfs某一路径下，文件个数(加上了查询的根路径)和大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fsck &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;ori_partitioned</span><br></pre></td></tr></table></figure>

<h2 id="hdfs常用命令"><a href="#hdfs常用命令" class="headerlink" title="hdfs常用命令:"></a>hdfs常用命令:</h2><p>1.hdfs dfsadmin 打开hdfs管理员客户端</p>
<ul>
<li><p>hdfs dfsadmin -report  可以看系统健康状况，好处是我可以写shell脚本定时采集这个数据，将数据传输到监控平台进行数据上报达到监控</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">Configured Capacity: 62608195584 (58.31 GB)</span><br><span class="line">Present Capacity: 9957312247 (9.27 GB)</span><br><span class="line">DFS Remaining: 6733078528 (6.27 GB)</span><br><span class="line">DFS Used: 3224233719 (3.00 GB)</span><br><span class="line">DFS Used%: 32.38%</span><br><span class="line">Under replicated blocks: 153</span><br><span class="line">Blocks with corrupt replicas: 0</span><br><span class="line">Missing blocks: 0</span><br><span class="line">Missing blocks (with replication factor 1): 0</span><br><span class="line"></span><br><span class="line">-------------------------------------------------</span><br><span class="line">Live datanodes (3):</span><br><span class="line"></span><br><span class="line">Name: 192.168.1.103:50010 (hadoop103)</span><br><span class="line">Hostname: hadoop103</span><br><span class="line">Decommission Status : Normal</span><br><span class="line">Configured Capacity: 20869398528 (19.44 GB)</span><br><span class="line">DFS Used: 1164574636 (1.08 GB)</span><br><span class="line">Non DFS Used: 17228857428 (16.05 GB)</span><br><span class="line">DFS Remaining: 2475966464 (2.31 GB)</span><br><span class="line">DFS Used%: 5.58%</span><br><span class="line">DFS Remaining%: 11.86%</span><br><span class="line">Configured Cache Capacity: 0 (0 B)</span><br><span class="line">Cache Used: 0 (0 B)</span><br><span class="line">Cache Remaining: 0 (0 B)</span><br><span class="line">Cache Used%: 100.00%</span><br><span class="line">Cache Remaining%: 0.00%</span><br><span class="line">Xceivers: 1</span><br><span class="line">Last contact: Thu May 28 19:24:37 CST 2020</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Name: 192.168.1.104:50010 (hadoop104)</span><br><span class="line">Hostname: hadoop104</span><br><span class="line">Decommission Status : Normal</span><br><span class="line">Configured Capacity: 20869398528 (19.44 GB)</span><br><span class="line">DFS Used: 1027734441 (980.12 MB)</span><br><span class="line">Non DFS Used: 17687430231 (16.47 GB)</span><br><span class="line">DFS Remaining: 2154233856 (2.01 GB)</span><br><span class="line">DFS Used%: 4.92%</span><br><span class="line">DFS Remaining%: 10.32%</span><br><span class="line">Configured Cache Capacity: 0 (0 B)</span><br><span class="line">Cache Used: 0 (0 B)</span><br><span class="line">Cache Remaining: 0 (0 B)</span><br><span class="line">Cache Used%: 100.00%</span><br><span class="line">Cache Remaining%: 0.00%</span><br><span class="line">Xceivers: 1</span><br><span class="line">Last contact: Thu May 28 19:24:38 CST 2020</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Name: 192.168.1.102:50010 (hadoop102)</span><br><span class="line">Hostname: hadoop102</span><br><span class="line">Decommission Status : Normal</span><br><span class="line">Configured Capacity: 20869398528 (19.44 GB)</span><br><span class="line">DFS Used: 1031924642 (984.12 MB)</span><br><span class="line">Non DFS Used: 17734595678 (16.52 GB)</span><br><span class="line">DFS Remaining: 2102878208 (1.96 GB)</span><br><span class="line">DFS Used%: 4.94%</span><br><span class="line">DFS Remaining%: 10.08%</span><br><span class="line">Configured Cache Capacity: 0 (0 B)</span><br><span class="line">Cache Used: 0 (0 B)</span><br><span class="line">Cache Remaining: 0 (0 B)</span><br><span class="line">Cache Used%: 100.00%</span><br><span class="line">Cache Remaining%: 0.00%</span><br><span class="line">Xceivers: 1</span><br><span class="line">Last contact: Thu May 28 19:24:37 CST 2020</span><br></pre></td></tr></table></figure>
</li>
<li><p>hdfs dfsadmin [-safemode &lt;enter | leave | get | wait&gt;]</p>
<ul>
<li>安全模式，集群进入安全模式不能写数据，但是能读数据。错误:Name node is in safe mode。手动关闭安全模式：hdfs dfsadmin -safemode leave</li>
<li>什么时候会进入安全模式？<ul>
<li>（故障场景）hdfs发生了故障，检查nn的log日志发现进入了安全模式，看什么原因进入的安全模式，之后尝试手动离开安全模式。</li>
<li>（业务场景）集群维护时候，让上游数据截断，并且让nn进入安全模式，可以读不能写。这是双重保障。尽量不要关闭集群，不然可能再次启动就启动不起来了</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="HDFS数据不均衡的两个场景"><a href="#HDFS数据不均衡的两个场景" class="headerlink" title="HDFS数据不均衡的两个场景"></a>HDFS数据不均衡的两个场景</h1><h2 id="各个DN节点的数据平衡（注意这是集群数据不均衡而不是单个DN的）"><a href="#各个DN节点的数据平衡（注意这是集群数据不均衡而不是单个DN的）" class="headerlink" title="各个DN节点的数据平衡（注意这是集群数据不均衡而不是单个DN的）"></a>各个DN节点的数据平衡（注意这是集群数据不均衡而不是单个DN的）</h2><p>&emsp;&emsp; 比如集群有三个节点分别是dn1，dn2，dn3并且硬盘大小都相同。其中dn1使用了硬盘的90%，dn2使用了硬盘的60%，dn3使用了硬盘的80%。<br>&emsp;&emsp; hdfs提供了一个脚本，是/home/hadoop/app/hadoop/sbin目录下的start-balancer.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop001 sbin]$ .&#x2F;start-balancer.sh -threshold 10.0</span><br><span class="line">starting balancer， logging to &#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop&#x2F;logs&#x2F;hadoop-hadoop(用户)-balancer-hadoop001.out</span><br></pre></td></tr></table></figure>
<p>参数threshold(阈值)含义: 每个节点的磁盘使用率 - 平均的磁盘使用率 &lt; threshold%<br>&emsp;&emsp; 例:90+60+80=230/3=76%<br>&emsp;&emsp; &emsp;&emsp; 90%-76%=14% &gt; 10%   这里面数据要移出来部分<br>&emsp;&emsp; &emsp;&emsp; 60%-76%=-16% &lt; 10%  其他节点的数据可以转移过来<br>&emsp;&emsp; &emsp;&emsp; 80%-76%=4% &lt; 10%    其他节点的数据可以转移过来<br>一共多14%+4%=18% ， 可以将这18%都转移到60%的这台机器上<br>数据均衡后 使用率分别为(假设每个DN节点的存储空间都一样，如果不一样还需要注意传过来的14%的数据，过来后是否会撑爆掉机器)  76%(90-14)，78%(60+18)，76%(80-4)<br>     76-76=0 &lt; 10<br>     78-76=2 &lt; 10<br>     76-76=0 &lt; 10<br>均满足要求</p>
<p><em>生产上从现在开始 ./start-balancer.sh -threshold 10.0  放到业务低谷比如凌晨，每天定时去做平衡操作。如果平常不做，等最后再做平衡会有大量数据操作会造成大量的硬盘IO，网络io可能让服务器资源卡住，会影响业务</em></p>
<p>调整平衡的网络带宽  ，hdfs-site.xml文件<br>dfs.datanode.balance.bandwidthPerSec 默认参数10m–&gt;调整为50m</p>
<h2 id="单个DN的多块磁盘的数据均衡"><a href="#单个DN的多块磁盘的数据均衡" class="headerlink" title="单个DN的多块磁盘的数据均衡"></a>单个DN的多块磁盘的数据均衡</h2><p>为什么要用多块物理磁盘?</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">多个磁盘的io是叠加的，比如每块磁盘io每秒能写10M，那么我现在有3块磁盘，则我1s能写30M数据</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp; 在投产前规划单个DN机器，配置多个磁盘路径，在公司买机器的时候，一定要说清楚，不要等后期再来做数据迁移的这些事情，磁盘也不是很贵(<em>生产上性价比最高的是西部数据或者希捷2.5英寸，10000转/分，2T</em>，因为超过2T是很贵的)，我们前期就先规划两年到三年的存储，不要一块磁盘一块磁盘的加。</p>
<p>配置参数:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;&#x2F;data01，&#x2F;data02，&#x2F;data03&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp; 多块磁盘在写的时候会自己均衡，基本不会出现很大的偏差，所以如果不新加磁盘的话半个月均衡一次就可以了。但是假如目前该DN磁盘容量不够了，需要添加新的磁盘这时候就要将该DN上的磁盘做数据均衡<br>想做单个DN的磁盘数据均衡需要打开配置参数，配置文件hdfs-site.xml:<br>查看hadoop官网Documentation可以看到apache版本的在3.X以上才加入这个参数，但是大部分小伙伴公司是2.x所以这个特性用不了！！！<br>&emsp;&emsp; 但是cdh版本的hadoop-2.6.0-cdh5.16.2有此功能，虽然hadoop版本是hadoop-2.6.0，但是CDH会把hadoop的新特性的地方加载到自己的本地，但是原有的版本号不会变化，相当于打了很多补丁，所以支持dfs.disk.balance<br>&emsp;&emsp; CDH网站：<a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.16.2/</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.disk.balancer.enabled&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<p>之后分别执行命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hdfs diskbalancer  -plan hadoop102(当前机器名) </span><br><span class="line">如果不需要均衡会提示:</span><br><span class="line">20&#x2F;05&#x2F;12 23:25:48 INFO command.Command: No plan generated. </span><br><span class="line">DiskBalancing not needed for node: hadoop102 threshold used: 10.0</span><br><span class="line"></span><br><span class="line">如果需要均衡会生成文件:  hadoop102.plan.json文件</span><br><span class="line"></span><br><span class="line">执行这个文件: hdfs diskbalancer -execute hadoop102.plan.json </span><br><span class="line"></span><br><span class="line">查询: hdfs diskbalancer  -query hadoop102</span><br></pre></td></tr></table></figure>

<p>有10块磁盘(比如每个磁盘2T)那么一共有20T，不做raid(运维的，磁盘高可靠但是会降低存储空间，大数据是不做raid的，因为有副本机制，丢了也能自动恢复)<br>磁盘的配置都是linux运维做的</p>
<p>场景：第一个月上了一个磁盘已经使用480G，第二个月新增一个磁盘2T，有两种方法，把第一个磁盘的数据直接mv到第二个磁盘然后在第一个磁盘原先存放数据处做软连接指向第二个磁盘新移动数据的位置。但是如果新加的磁盘也是500G移动过来肯定是不行的。所以第二种方法就是多个磁盘做数据均衡</p>
<h1 id="HDFS上小文件的处理"><a href="#HDFS上小文件的处理" class="headerlink" title="HDFS上小文件的处理"></a>HDFS上小文件的处理</h1><h2 id="小文件影响"><a href="#小文件影响" class="headerlink" title="小文件影响"></a>小文件影响</h2><ul>
<li>1 个文件块，占用 namenode 内存大概 150 字节。一个小文件，即使不达到128M也会存储单独一块。所以如果小文件过多会对nn内存造成很大压力。</li>
<li>影响计算引擎的任务数量，比如每个小的文件都会生成一个Map任务</li>
</ul>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h3><p>&emsp;&emsp;采用 har 归档方式，将小文件归档</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">归档：</span><br><span class="line"></span><br><span class="line">    bin&#x2F;hadoop archive -archiveName myhar.har -p  &#x2F;user&#x2F;atguigu     &#x2F;user&#x2F;my</span><br><span class="line">        归档                        归档名称      要归档哪一个文件  归档后放到哪</span><br><span class="line"></span><br><span class="line">查看归档：</span><br><span class="line"></span><br><span class="line">    hadoop fs -lsr har:&#x2F;&#x2F;&#x2F;input.har</span><br><span class="line"></span><br><span class="line">解归档文件</span><br><span class="line"></span><br><span class="line">    hadoop fs -cp har:&#x2F;&#x2F;&#x2F; user&#x2F;my&#x2F;myhar.har &#x2F;* &#x2F;user&#x2F;atguigu</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;注意打成har 文件可以大大降低namenode 的内存压力。但对于MapReduce 来说起不到任何作用，因为har文件就相当一个目录，仍然不能将小文件合并到一个split中去，一个小文件一个split ，仍然是低效的</p>
<h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><p>&emsp;&emsp;采用ConbinFileInputFormat来作为输入 , 它能减少切片个数比如把三个小文件化为一个切片 , 最终就是减少了maptask的个数。</p>
<h3 id="方法三"><a href="#方法三" class="headerlink" title="方法三"></a>方法三</h3><p>&emsp;&emsp;有小文件场景开启 JVM 重用；如果没有小文件，不要开启 JVM 重用，因为会一直占用使用到的 task 卡槽，直到任务完成才释放。<br>&emsp;&emsp;JVM 重用可以使得 JVM 实例在同一个 job 中重新使用 N 次，N 的值可以在 Hadoop 的mapred-site.xml 文件中进行配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.job.jvm.numtasks&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;10&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;How many tasks to run per jvm,if set to - 1 ,there is no limit&lt;&#x2F;description&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<h3 id="方法四"><a href="#方法四" class="headerlink" title="方法四"></a>方法四</h3><p>&emsp;&emsp;如果数据已经在hdfs上了，就是定时去合并不常用的文件。（因为数仓一般都是分区的，如果合并当天分区的数据可能业务正在读取，此时合并可能会读取数据失败。所以定时的，在业务低谷时候合并冷数据。生产上合并7(或15或30)天之前的一天数据，如今天20号那么合并14号当天的小文件）</p>
<p>&emsp;&emsp;经验值：10M以下的文件认为小文件，一般将小文件合并到120M文件并不会完全合并到块大小（快大小默认128M），因为如果在程序合并设置为128M可能真正合并的时候就超过了128M（因为要保证文件内容完整性，可能最后一个文件合并后会超过128M）比如合并到129M则此时又会分成两块（128M，1M）其中1M又是小文件了</p>
<h2 id="结束了"><a href="#结束了" class="headerlink" title="结束了"></a>结束了</h2>
    </div>

    <div>全文完。</div>
  </article>
  <div class="toc-container">
    <div id="toc-div" class="toc-article" >
   <strong class="toc-title">Index</strong>
     
       <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS组成"><span class="toc-text">HDFS组成</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#NameNode"><span class="toc-text">NameNode</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#镜像文件（fsimage）"><span class="toc-text">镜像文件（fsimage）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编辑日志（edits）"><span class="toc-text">编辑日志（edits）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#seen-txid"><span class="toc-text">seen_txid</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nn工作流程"><span class="toc-text">nn工作流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataNode"><span class="toc-text">DataNode</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#块-block"><span class="toc-text">块 block</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#块和切片的区别"><span class="toc-text">块和切片的区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#块大小"><span class="toc-text">块大小</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#副本数"><span class="toc-text">副本数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs写流程"><span class="toc-text">hdfs写流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs读流程"><span class="toc-text">hdfs读流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hdfs副本放置策略-（面试和生产都需要）"><span class="toc-text">hdfs副本放置策略 （面试和生产都需要）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SecondaryNamenode"><span class="toc-text">SecondaryNamenode</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS命令"><span class="toc-text">HDFS命令</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#hadoop常用命令"><span class="toc-text">hadoop常用命令:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hdfs常用命令"><span class="toc-text">hdfs常用命令:</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS数据不均衡的两个场景"><span class="toc-text">HDFS数据不均衡的两个场景</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#各个DN节点的数据平衡（注意这是集群数据不均衡而不是单个DN的）"><span class="toc-text">各个DN节点的数据平衡（注意这是集群数据不均衡而不是单个DN的）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#单个DN的多块磁盘的数据均衡"><span class="toc-text">单个DN的多块磁盘的数据均衡</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS上小文件的处理"><span class="toc-text">HDFS上小文件的处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#小文件影响"><span class="toc-text">小文件影响</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#解决方法"><span class="toc-text">解决方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#方法一"><span class="toc-text">方法一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#方法二"><span class="toc-text">方法二</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#方法三"><span class="toc-text">方法三</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#方法四"><span class="toc-text">方法四</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#结束了"><span class="toc-text">结束了</span></a></li></ol></li></ol>
     
</div>
  </div>
</div>
<div class="copyright">
    <span>本作品采用</span>
    <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener">知识共享署名 4.0 国际许可协议</a>
    <span>进行许可。 转载时请注明原文链接。</span>
</div>
<div class="share" style="width: 100%;">
  <img src="https://kevinofneu-blog-static.oss-cn-beijing.aliyuncs.com/static/2018-12-10-qrcode_for_gh_ffacf5722095_258.jpg" alt="Running Geek" style="margin: auto; display: block;"/>

  <div style="margin: auto; text-align: center; font-size: 0.8em; color: grey;">老铁们关注走一走，不迷路</div>
  
</div>

  
    <div class="post-nav">
      <div class="post-nav-item post-nav-next">
        
          <span>〈 </span>
          <a href="/2016/05/21/hadoop/hadoop/" rel="next" title="hadoop介绍">
          hadoop介绍
          </a>
        
      </div>
  
      <div class="post-nav-item post-nav-prev">
          
          <a href="/2016/06/23/hadoop/MapReduce/" rel="prev" title="MapReduce">
            MapReduce
          </a>
          <span>〉</span>
        
      </div>
    </div>
  


    </div>

    

  </div>
  <footer class="footer text-center">
    <div id="bottom-inner">
        <a class="bottom-item" href="https://blog.0xff000000.com" target="_blank" rel="noopener">首页</a> |
        <a class="bottom-item" href="https://0xff000000.com" target="_blank">主站</a> |
        <a class="bottom-item" href="https://github.com/KevinOfNeu" target="_blank">GitHub</a> |
        <a class="bottom-item" href="https://hexo.io" target="_blank">Powered by hexo</a> |
        <a class="bottom-item" href="https://github.com/KevinOfNeu/hexo-theme-xoxo" target="_blank">Theme xoxo</a>
    </div>
</footer>
  

<script>
  (function(window, document, undefined) {

    var timer = null;

    function returnTop() {
      cancelAnimationFrame(timer);
      timer = requestAnimationFrame(function fn() {
        var oTop = document.body.scrollTop || document.documentElement.scrollTop;
        if (oTop > 0) {
          document.body.scrollTop = document.documentElement.scrollTop = oTop - 50;
          timer = requestAnimationFrame(fn);
        } else {
          cancelAnimationFrame(timer);
        }
      });
    }

    var hearts = [];
    window.requestAnimationFrame = (function() {
      return window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.oRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        function(callback) {
          setTimeout(callback, 1000 / 60);
        }
    })();
    init();

    function init() {
      css(".heart{z-index:9999;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: absolute;}.heart:after{top: -5px;}.heart:before{left: -5px;}");
      attachEvent();
      gameloop();
      addMenuEvent();
    }

    function gameloop() {
      for (var i = 0; i < hearts.length; i++) {
        if (hearts[i].alpha <= 0) {
          document.body.removeChild(hearts[i].el);
          hearts.splice(i, 1);
          continue;
        }
        hearts[i].y--;
        hearts[i].scale += 0.004;
        hearts[i].alpha -= 0.013;
        hearts[i].el.style.cssText = "left:" + hearts[i].x + "px;top:" + hearts[i].y + "px;opacity:" + hearts[i].alpha + ";transform:scale(" + hearts[i].scale + "," + hearts[i].scale + ") rotate(45deg);background:" + hearts[i].color;
      }
      requestAnimationFrame(gameloop);
    }

    /**
     * 给logo设置点击事件
     * 
     * - 回到顶部
     * - 出现爱心
     */
    function attachEvent() {
      var old = typeof window.onclick === "function" && window.onclick;
      var logo = document.getElementById("logo");
      if (logo) {
        logo.onclick = function(event) {
          returnTop();
          old && old();
          createHeart(event);
        }
      }
      
    }

    function createHeart(event) {
      var d = document.createElement("div");
      d.className = "heart";
      hearts.push({
        el: d,
        x: event.clientX - 5,
        y: event.clientY - 5,
        scale: 1,
        alpha: 1,
        color: randomColor()
      });
      document.body.appendChild(d);
    }

    function css(css) {
      var style = document.createElement("style");
      style.type = "text/css";
      try {
        style.appendChild(document.createTextNode(css));
      } catch (ex) {
        style.styleSheet.cssText = css;
      }
      document.getElementsByTagName('head')[0].appendChild(style);
    }

    function randomColor() {
      // return "rgb(" + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + "," + (~~(Math.random() * 255)) + ")";
      return "#F44336";
    }

    function addMenuEvent() {
      var menu = document.getElementById('menu-main-post');
      if (menu) {
        var toc = document.getElementById('toc');
        if (toc) {
          menu.onclick = function() {
            if (toc) {
              if (toc.style.display == 'block') {
                toc.style.display = 'none';
              } else {
                toc.style.display = 'block';
              }
            }
          };
        } else {
          menu.style.display = 'none';
        }
      }
    }

  })(window, document);
</script>

  



</body>
</html>
